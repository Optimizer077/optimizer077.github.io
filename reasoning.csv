"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"HXY94F8Q","preprint","2024","Cao, Jin; Meng, Deyu; Cao, Xiangyong","Chain-of-Restoration: Multi-Task Image Restoration Models are Zero-Shot Step-by-Step Universal Image Restorers","","","","10.48550/arXiv.2410.08688","http://arxiv.org/abs/2410.08688","Despite previous image restoration (IR) methods have often concentrated on isolated degradations, recent research has increasingly focused on addressing composite degradations involving a complex combination of multiple isolated degradations. However, current IR methods for composite degradations require building training data that contain an exponential number of possible degradation combinations, which brings in a significant burden. To alleviate this issue, this paper proposes a new task setting, i.e. Universal Image Restoration (UIR). Specifically, UIR doesn't require training on all the degradation combinations but only on a set of degradation bases and then removing any degradation that these bases can potentially compose in a zero-shot manner. Inspired by the Chain-of-Thought that prompts large language models (LLMs) to address problems step-by-step, we propose Chain-of-Restoration (CoR) mechanism, which instructs models to remove unknown composite degradations step-by-step. By integrating a simple Degradation Discriminator into pre-trained multi-task models, CoR facilitates the process where models remove one degradation basis per step, continuing this process until the image is fully restored from the unknown composite degradation. Extensive experiments show that CoR can significantly improve model performance in removing composite degradations, achieving comparable or better results than those state-of-the-art (SoTA) methods trained on all degradations.","2024-12-04","2025-04-21 04:29:30","2025-04-21 04:29:30","2025-04-21 04:29:30","","","","","","","Chain-of-Restoration","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2410.08688 [cs]","","/Users/alihaider/Zotero/storage/5G5D54DQ/Cao et al. - 2024 - Chain-of-Restoration Multi-Task Image Restoration Models are Zero-Shot Step-by-Step Universal Image.pdf; /Users/alihaider/Zotero/storage/HCW6PXLV/2410.html","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2410.08688","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7RJLL7YN","preprint","2025","Zhang, Anqi; Chen, Yulin; Pan, Jane; Zhao, Chen; Panda, Aurojit; Li, Jinyang; He, He","Reasoning Models Know When They're Right: Probing Hidden States for Self-Verification","","","","10.48550/arXiv.2504.05419","http://arxiv.org/abs/2504.05419","Reasoning models have achieved remarkable performance on tasks like math and logical reasoning thanks to their ability to search during reasoning. However, they still suffer from overthinking, often performing unnecessary reasoning steps even after reaching the correct answer. This raises the question: can models evaluate the correctness of their intermediate answers during reasoning? In this work, we study whether reasoning models encode information about answer correctness through probing the model's hidden states. The resulting probe can verify intermediate answers with high accuracy and produces highly calibrated scores. Additionally, we find models' hidden states encode correctness of future answers, enabling early prediction of the correctness before the intermediate answer is fully formulated. We then use the probe as a verifier to decide whether to exit reasoning at intermediate answers during inference, reducing the number of inference tokens by 24\% without compromising performance. These findings confirm that reasoning models do encode a notion of correctness yet fail to exploit it, revealing substantial untapped potential to enhance their efficiency.","2025-04-07","2025-04-21 04:32:37","2025-04-21 04:32:37","2025-04-21 04:32:37","","","","","","","Reasoning Models Know When They're Right","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2504.05419 [cs]","","/Users/alihaider/Zotero/storage/9QJ3KCLR/Zhang et al. - 2025 - Reasoning Models Know When They're Right Probing Hidden States for Self-Verification.pdf; /Users/alihaider/Zotero/storage/UIVX2NQ3/2504.html","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2504.05419","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7P4K6IYM","preprint","2025","Wang, Shangshang; Asilis, Julian; Akgül, Ömer Faruk; Bilgin, Enes Burak; Liu, Ollie; Neiswanger, Willie","Tina: Tiny Reasoning Models via LoRA","","","","10.48550/arXiv.2504.15777","http://arxiv.org/abs/2504.15777","How cost-effectively can strong reasoning abilities be achieved in language models? Driven by this fundamental question, we present Tina, a family of tiny reasoning models achieved with high cost-efficiency. Notably, Tina demonstrates that substantial reasoning performance can be developed using only minimal resources, by applying parameter-efficient updates during reinforcement learning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B parameter base model. This minimalist approach produces models that achieve reasoning performance which is competitive with, and sometimes surpasses, SOTA RL reasoning models built upon the same base model. Crucially, this is achieved at a tiny fraction of the computational post-training cost employed by existing SOTA models. In fact, the best Tina model achieves a >20\% reasoning performance increase and 43.33\% Pass@1 accuracy on AIME24, at only \$9 USD post-training and evaluation cost (i.e., an estimated 260x cost reduction). Our work reveals the surprising effectiveness of efficient RL reasoning via LoRA. We validate this across multiple open-source reasoning datasets and various ablation settings starting with a single, fixed set of hyperparameters. Furthermore, we hypothesize that this effectiveness and efficiency stem from LoRA rapidly adapting the model to the structural format of reasoning rewarded by RL, while largely preserving the base model's underlying knowledge. In service of accessibility and open research, we fully open-source all code, training logs, and model weights \& checkpoints.","2025-04-22","2025-04-24 03:08:58","2025-04-24 03:08:58","2025-04-24 03:08:58","","","","","","","Tina","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2504.15777 [cs]","","/Users/alihaider/Zotero/storage/HEQAR6PZ/Wang et al. - 2025 - Tina Tiny Reasoning Models via LoRA.pdf; /Users/alihaider/Zotero/storage/UCJ2GUJ7/2504.html","","","Computer Science - Machine Learning; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2504.15777","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z3R3UIHU","preprint","2025","Chen, Qiguang; Qin, Libo; Liu, Jinhao; Peng, Dengyun; Guan, Jiannan; Wang, Peng; Hu, Mengkang; Zhou, Yuhang; Gao, Te; Che, Wanxiang","Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models","","","","10.48550/arXiv.2503.09567","http://arxiv.org/abs/2503.09567","Recent advancements in reasoning with large language models (RLLMs), such as OpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in complex domains like mathematics and coding. A central factor in their success lies in the application of long chain-of-thought (Long CoT) characteristics, which enhance reasoning abilities and enable the solution of intricate problems. However, despite these developments, a comprehensive survey on Long CoT is still lacking, limiting our understanding of its distinctions from traditional short chain-of-thought (Short CoT) and complicating ongoing debates on issues like ""overthinking"" and ""test-time scaling."" This survey seeks to fill this gap by offering a unified perspective on Long CoT. (1) We first distinguish Long CoT from Short CoT and introduce a novel taxonomy to categorize current reasoning paradigms. (2) Next, we explore the key characteristics of Long CoT: deep reasoning, extensive exploration, and feasible reflection, which enable models to handle more complex tasks and produce more efficient, coherent outcomes compared to the shallower Short CoT. (3) We then investigate key phenomena such as the emergence of Long CoT with these characteristics, including overthinking, and test-time scaling, offering insights into how these processes manifest in practice. (4) Finally, we identify significant research gaps and highlight promising future directions, including the integration of multi-modal reasoning, efficiency improvements, and enhanced knowledge frameworks. By providing a structured overview, this survey aims to inspire future research and further the development of logical reasoning in artificial intelligence.","2025-04-09","2025-04-27 23:02:52","2025-04-27 23:02:52","2025-04-27 23:02:52","","","","","","","Towards Reasoning Era","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2503.09567 [cs]","","/Users/alihaider/Zotero/storage/PHWX633C/Chen et al. - 2025 - Towards Reasoning Era A Survey of Long Chain-of-Thought for Reasoning Large Language Models.pdf; /Users/alihaider/Zotero/storage/I7FABPSX/2503.html","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2503.09567","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KE7B3676","preprint","2024","Wu, Siwei; Peng, Zhongyuan; Du, Xinrun; Zheng, Tuney; Liu, Minghao; Wu, Jialong; Ma, Jiachen; Li, Yizhi; Yang, Jian; Zhou, Wangchunshu; Lin, Qunshu; Zhao, Junbo; Zhang, Zhaoxiang; Huang, Wenhao; Zhang, Ge; Lin, Chenghua; Liu, J. H.","A Comparative Study on Reasoning Patterns of OpenAI's o1 Model","","","","10.48550/arXiv.2410.13639","http://arxiv.org/abs/2410.13639","Enabling Large Language Models (LLMs) to handle a wider range of complex tasks (e.g., coding, math) has drawn great attention from many researchers. As LLMs continue to evolve, merely increasing the number of model parameters yields diminishing performance improvements and heavy computational costs. Recently, OpenAI's o1 model has shown that inference strategies (i.e., Test-time Compute methods) can also significantly enhance the reasoning capabilities of LLMs. However, the mechanisms behind these methods are still unexplored. In our work, to investigate the reasoning patterns of o1, we compare o1 with existing Test-time Compute methods (BoN, Step-wise BoN, Agent Workflow, and Self-Refine) by using OpenAI's GPT-4o as a backbone on general reasoning benchmarks in three domains (i.e., math, coding, commonsense reasoning). Specifically, first, our experiments show that the o1 model has achieved the best performance on most datasets. Second, as for the methods of searching diverse responses (e.g., BoN), we find the reward models' capability and the search space both limit the upper boundary of these methods. Third, as for the methods that break the problem into many sub-problems, the Agent Workflow has achieved better performance than Step-wise BoN due to the domain-specific system prompt for planning better reasoning processes. Fourth, it is worth mentioning that we have summarized six reasoning patterns of o1, and provided a detailed analysis on several reasoning benchmarks.","2024-10-17","2025-04-28 05:23:05","2025-04-28 05:23:08","2025-04-28 05:23:05","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2410.13639 [cs] version: 1","","/Users/alihaider/Zotero/storage/BFIWD2X7/Wu et al. - 2024 - A Comparative Study on Reasoning Patterns of OpenAI's o1 Model.pdf; /Users/alihaider/Zotero/storage/IQPG4QCB/2410.html","","","Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2410.13639","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RHDQENMC","preprint","2025","Darlow, Luke; Regan, Ciaran; Risi, Sebastian; Seely, Jeffrey; Jones, Llion","Continuous Thought Machines","","","","10.48550/arXiv.2505.05522","http://arxiv.org/abs/2505.05522","Biological brains demonstrate complex neural activity, where the timing and interplay between neurons is critical to how brains process information. Most deep learning architectures simplify neural activity by abstracting away temporal dynamics. In this paper we challenge that paradigm. By incorporating neuron-level processing and synchronization, we can effectively reintroduce neural timing as a foundational element. We present the Continuous Thought Machine (CTM), a model designed to leverage neural dynamics as its core representation. The CTM has two core innovations: (1) neuron-level temporal processing, where each neuron uses unique weight parameters to process a history of incoming signals; and (2) neural synchronization employed as a latent representation. The CTM aims to strike a balance between oversimplified neuron abstractions that improve computational efficiency, and biological realism. It operates at a level of abstraction that effectively captures essential temporal dynamics while remaining computationally tractable for deep learning. We demonstrate the CTM's strong performance and versatility across a range of challenging tasks, including ImageNet-1K classification, solving 2D mazes, sorting, parity computation, question-answering, and RL tasks. Beyond displaying rich internal representations and offering a natural avenue for interpretation owing to its internal process, the CTM is able to perform tasks that require complex sequential reasoning. The CTM can also leverage adaptive compute, where it can stop earlier for simpler tasks, or keep computing when faced with more challenging instances. The goal of this work is to share the CTM and its associated innovations, rather than pushing for new state-of-the-art results. To that end, we believe the CTM represents a significant step toward developing more biologically plausible and powerful artificial intelligence systems.","2025-05-08","2025-05-12 03:15:02","2025-05-12 03:15:09","2025-05-12 03:15:02","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2505.05522 [cs]","","/Users/alihaider/Zotero/storage/6S2RK7MP/Darlow et al. - 2025 - Continuous Thought Machines.pdf; /Users/alihaider/Zotero/storage/MJ85DHWG/2505.html","","","Computer Science - Artificial Intelligence; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2505.05522","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TDLBIVYL","preprint","2024","Chen, Lichang; Hu, Hexiang; Zhang, Mingda; Chen, Yiwen; Wang, Zifeng; Li, Yandong; Shyam, Pranav; Zhou, Tianyi; Huang, Heng; Yang, Ming-Hsuan; Gong, Boqing","OmnixR: Evaluating Omni-modality Language Models on Reasoning across Modalities","","","","10.48550/arXiv.2410.12219","http://arxiv.org/abs/2410.12219","We introduce OmnixR, an evaluation suite designed to benchmark SoTA Omni-modality Language Models, such as GPT-4o and Gemini. Evaluating OLMs, which integrate multiple modalities such as text, vision, and audio, presents unique challenges. Particularly, the user message might often consist of multiple modalities, such that OLMs have to establish holistic understanding and reasoning across modalities to accomplish the task. Existing benchmarks are limited to single modality or dual-modality tasks, overlooking comprehensive multi-modal assessments of model reasoning. To address this, OmnixR offers two evaluation variants: (1)synthetic subset: a synthetic dataset generated automatically by translating text into multiple modalities--audio, images, video, and hybrids (Omnify). (2)realistic subset: a real-world dataset, manually curated and annotated by experts, for evaluating cross-modal reasoning in natural settings. OmnixR presents a unique evaluation towards assessing OLMs over a diverse mix of modalities, such as a question that involves video, audio, and text, providing a rigorous cross-modal reasoning testbed unlike any existing benchmarks. Our experiments find that all state-of-the-art OLMs struggle with OmnixR questions that require integrating information from multiple modalities to answer. Further analysis highlights differences in reasoning behavior, underscoring the challenges of omni-modal AI alignment.","2024-10-16","2025-06-03 12:57:14","2025-06-03 12:57:33","2025-06-03 12:57:14","","","","","","","OmnixR","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2410.12219 [cs] version: 1","","/Users/alihaider/Zotero/storage/WF3GD4HK/Chen et al. - 2024 - OmnixR Evaluating Omni-modality Language Models on Reasoning across Modalities.pdf; /Users/alihaider/Zotero/storage/RAJI5W6Y/2410.html","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Multimedia","","","","","","","","","","","","","","","","","","","arXiv:2410.12219","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NBL5VBMB","preprint","2025","Zhou, Yingjie; Cao, Jiezhang; Zhang, Zicheng; Wen, Farong; Jiang, Yanwei; Jia, Jun; Liu, Xiaohong; Min, Xiongkuo; Zhai, Guangtao","Q-Agent: Quality-Driven Chain-of-Thought Image Restoration Agent through Robust Multimodal Large Language Model","","","","10.48550/arXiv.2504.07148","http://arxiv.org/abs/2504.07148","Image restoration (IR) often faces various complex and unknown degradations in real-world scenarios, such as noise, blurring, compression artifacts, and low resolution, etc. Training specific models for specific degradation may lead to poor generalization. To handle multiple degradations simultaneously, All-in-One models might sacrifice performance on certain types of degradation and still struggle with unseen degradations during training. Existing IR agents rely on multimodal large language models (MLLM) and a time-consuming rolling-back selection strategy neglecting image quality. As a result, they may misinterpret degradations and have high time and computational costs to conduct unnecessary IR tasks with redundant order. To address these, we propose a Quality-Driven agent (Q-Agent) via Chain-of-Thought (CoT) restoration. Specifically, our Q-Agent consists of robust degradation perception and quality-driven greedy restoration. The former module first fine-tunes MLLM, and uses CoT to decompose multi-degradation perception into single-degradation perception tasks to enhance the perception of MLLMs. The latter employs objective image quality assessment (IQA) metrics to determine the optimal restoration sequence and execute the corresponding restoration algorithms. Experimental results demonstrate that our Q-Agent achieves superior IR performance compared to existing All-in-One models.","2025-04-09","2025-06-04 05:41:19","2025-06-04 05:41:22","2025-06-04 05:41:19","","","","","","","Q-Agent","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2504.07148 [eess]","","/Users/alihaider/Zotero/storage/NX7DFCMC/Zhou et al. - 2025 - Q-Agent Quality-Driven Chain-of-Thought Image Restoration Agent through Robust Multimodal Large Lan.pdf; /Users/alihaider/Zotero/storage/VEIMRFU5/2504.html","","","Electrical Engineering and Systems Science - Image and Video Processing","","","","","","","","","","","","","","","","","","","arXiv:2504.07148","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BPBQJ38I","preprint","2025","Li, Bingchen; Li, Xin; Lu, Yiting; Chen, Zhibo","Hybrid Agents for Image Restoration","","","","10.48550/arXiv.2503.10120","http://arxiv.org/abs/2503.10120","Existing Image Restoration (IR) studies typically focus on task-specific or universal modes individually, relying on the mode selection of users and lacking the cooperation between multiple task-specific/universal restoration modes. This leads to insufficient interaction for unprofessional users and limits their restoration capability for complicated real-world applications. In this work, we present HybridAgent, intending to incorporate multiple restoration modes into a unified image restoration model and achieve intelligent and efficient user interaction through our proposed hybrid agents. Concretely, we propose the hybrid rule of fast, slow, and feedback restoration agents. Here, the slow restoration agent optimizes the powerful multimodal large language model (MLLM) with our proposed instruction-tuning dataset to identify degradations within images with ambiguous user prompts and invokes proper restoration tools accordingly. The fast restoration agent is designed based on a lightweight large language model (LLM) via in-context learning to understand the user prompts with simple and clear requirements, which can obviate the unnecessary time/resource costs of MLLM. Moreover, we introduce the mixed distortion removal mode for our HybridAgents, which is crucial but not concerned in previous agent-based works. It can effectively prevent the error propagation of step-by-step image restoration and largely improve the efficiency of the agent system. We validate the effectiveness of HybridAgent with both synthetic and real-world IR tasks.","2025-03-13","2025-06-04 05:42:31","2025-06-04 05:42:31","2025-06-04 05:42:31","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2503.10120 [cs]","","/Users/alihaider/Zotero/storage/CH5JWBIY/Li et al. - 2025 - Hybrid Agents for Image Restoration.pdf; /Users/alihaider/Zotero/storage/J56LQ4PE/2503.html","","","Computer Science - Computer Vision and Pattern Recognition; Electrical Engineering and Systems Science - Image and Video Processing","","","","","","","","","","","","","","","","","","","arXiv:2503.10120","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WA5K7RKJ","preprint","2025","Pan, Chenbin; He, Wenbin; Tu, Zhengzhong; Ren, Liu","DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models","","","","10.48550/arXiv.2505.24025","http://arxiv.org/abs/2505.24025","The recent explosive interest in the reasoning capabilities of large language models, such as DeepSeek-R1, has demonstrated remarkable success through reinforcement learning-based fine-tuning frameworks, exemplified by methods like Group Relative Policy Optimization (GRPO). However, such reasoning abilities remain underexplored and notably absent in vision foundation models, including representation models like the DINO series. In this work, we propose \textbf{DINO-R1}, the first such attempt to incentivize visual in-context reasoning capabilities of vision foundation models using reinforcement learning. Specifically, DINO-R1 introduces \textbf{Group Relative Query Optimization (GRQO)}, a novel reinforcement-style training strategy explicitly designed for query-based representation models, which computes query-level rewards based on group-normalized alignment quality. We also apply KL-regularization to stabilize the objectness distribution to reduce the training instability. This joint optimization enables dense and expressive supervision across queries while mitigating overfitting and distributional drift. Building upon Grounding-DINO, we train a series of DINO-R1 family models that integrate a visual prompt encoder and a visual-guided query selection mechanism. Extensive experiments on COCO, LVIS, and ODinW demonstrate that DINO-R1 significantly outperforms supervised fine-tuning baselines, achieving strong generalization in both open-vocabulary and closed-set visual prompting scenarios.","2025-05-29","2025-06-05 10:54:04","2025-06-05 10:54:06","2025-06-05 10:54:04","","","","","","","DINO-R1","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2505.24025 [cs]","","/Users/alihaider/Zotero/storage/9HDFXNF6/Pan et al. - 2025 - DINO-R1 Incentivizing Reasoning Capability in Vision Foundation Models.pdf; /Users/alihaider/Zotero/storage/TSD8VQ4E/2505.html","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2505.24025","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B77XEVB5","journalArticle","","Shojaee, Parshin; Mirzadeh, Iman; Alizadeh, Keivan; Horton, Maxwell; Bengio, Samy; Farajtabar, Mehrdad","The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity","","","","","","Recent generations of frontier language models have introduced Large Reasoning Models (LRMs) that generate detailed thinking processes before providing answers. While these models demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scaling properties, and limitations remain insufficiently understood. Current evaluations primarily focus on established mathematical and coding benchmarks, emphasizing final answer accuracy. However, this evaluation paradigm often suffers from data contamination and does not provide insights into the reasoning traces’ structure and quality. In this work, we systematically investigate these gaps with the help of controllable puzzle environments that allow precise manipulation of compositional complexity while maintaining consistent logical structures. This setup enables the analysis of not only final answers but also the internal reasoning traces, offering insights into how LRMs “think”. Through extensive experimentation across diverse puzzles, we show that frontier LRMs face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counterintuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then declines despite having an adequate token budget. By comparing LRMs with their standard LLM counterparts under equivalent inference compute, we identify three performance regimes: (1) lowcomplexity tasks where standard models surprisingly outperform LRMs, (2) medium-complexity tasks where additional thinking in LRMs demonstrates advantage, and (3) high-complexity tasks where both models experience complete collapse. We found that LRMs have limitations in exact computation: they fail to use explicit algorithms and reason inconsistently across puzzles. We also investigate the reasoning traces in more depth, studying the patterns of explored solutions and analyzing the models’ computational behavior, shedding light on their strengths, limitations, and ultimately raising crucial questions about their true reasoning capabilities.","","2025-06-09 01:47:40","2025-06-09 01:47:41","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/alihaider/Zotero/storage/9F24HKZX/Shojaee et al. - The Illusion of Thinking Understanding the Strengths and Limitations of Reasoning Models via the Le.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ASHU365P","preprint","2024","Springer, Jacob Mitchell; Kotha, Suhas; Fried, Daniel; Neubig, Graham; Raghunathan, Aditi","Repetition Improves Language Model Embeddings","","","","10.48550/arXiv.2402.15449","http://arxiv.org/abs/2402.15449","Recent approaches to improving the extraction of text embeddings from autoregressive large language models (LLMs) have largely focused on improvements to data, backbone pretrained language models, or improving task-differentiation via instructions. In this work, we address an architectural limitation of autoregressive models: token embeddings cannot contain information from tokens that appear later in the input. To address this limitation, we propose a simple approach, ""echo embeddings,"" in which we repeat the input twice in context and extract embeddings from the second occurrence. We show that echo embeddings of early tokens can encode information about later tokens, allowing us to maximally leverage high-quality LLMs for embeddings. On the MTEB leaderboard, echo embeddings improve over classical embeddings by over 9% zero-shot and by around 0.7% when fine-tuned. Echo embeddings with a Mistral-7B model achieve state-of-the-art compared to prior open source models that do not leverage synthetic fine-tuning data.","2024-02-23","2025-06-12 01:28:05","2025-06-12 01:28:05","2025-06-12 01:28:05","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2402.15449 [cs]","","/Users/alihaider/Zotero/storage/RDFWXLJP/Springer et al. - 2024 - Repetition Improves Language Model Embeddings.pdf","","","Computer Science - Machine Learning; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2402.15449","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BCPLBT5Z","journalArticle","2025","Liu, Zechun; Zhao, Changsheng; Fedorov, Igor; Soran, Bilge; Choudhary, Dhruv; Krishnamoorthi, Raghuraman; Chandra, Vikas; Tian, Yuandong; Blankevoort, Tijmen","SP I NQU A N T : LLM QUANTIZATION WITH LEARNED ROTATIONS","","","","","","Post-training quantization (PTQ) techniques applied to weights, activations, and the KV cache greatly reduce memory usage, latency, and power consumption of Large Language Models (LLMs), but may lead to large quantization errors when outliers are present. Rotating activation or weight matrices helps remove outliers and benefits quantization. In this work, we identify a collection of applicable rotation parameterizations that lead to identical outputs in full-precision Transformer architectures while enhancing quantization accuracy. In addition, we find that some random rotations lead to much better quantization than others, with an up to 13 points difference in downstream zero-shot reasoning performance. As a result, we propose SpinQuant, a novel approach that incorporates learned rotation matrices for optimal quantized network accuracy. With 4-bit quantization of weight, activation, and KV-cache, SpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full precision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by 19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also outperforms concurrent work QuaRot, which applies random rotations to remove outliers. In particular, for LLaMA-3 8B models that are hard to quantize, SpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot. Code is available at github.com/facebookresearch/SpinQuant.","2025","2025-06-12 01:44:58","2025-06-12 01:44:58","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/alihaider/Zotero/storage/NPZLEI5H/Liu et al. - 2025 - SP I NQU A N T  LLM QUANTIZATION WITH LEARNED ROTATIONS.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6BAYPBD5","preprint","2024","Feng, Yunlong; Xu, Yang; Qin, Libo; Wang, Yasheng; Che, Wanxiang","Improving Language Model Reasoning with Self-motivated Learning","","","","10.48550/arXiv.2404.07017","http://arxiv.org/abs/2404.07017","Large-scale high-quality training data is important for improving the performance of models. After trained with data that has rationales (reasoning steps), models gain reasoning capability. However, the dataset with high-quality rationales is relatively scarce due to the high annotation cost. To address this issue, we propose \textit{Self-motivated Learning} framework. The framework motivates the model itself to automatically generate rationales on existing datasets. Based on the inherent rank from correctness across multiple rationales, the model learns to generate better rationales, leading to higher reasoning capability. Specifically, we train a reward model with the rank to evaluate the quality of rationales, and improve the performance of reasoning through reinforcement learning. Experiment results of Llama2 7B on multiple reasoning datasets show that our method significantly improves the reasoning ability of models, even outperforming text-davinci-002 in some datasets.","2024-04-30","2025-06-27 09:25:17","2025-06-27 09:25:17","2025-06-27 09:25:17","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2404.07017 [cs]","","/Users/alihaider/Zotero/storage/RRPLLT92/Feng et al. - 2024 - Improving Language Model Reasoning with Self-motivated Learning.pdf; /Users/alihaider/Zotero/storage/2QMYBYPI/2404.html","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2404.07017","","","","","","","","","","","","","","","","","","","","","","","","","","",""