"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"Z8J2FPIJ","preprint","2024","Dunlap, Lisa; Zhang, Yuhui; Wang, Xiaohan; Zhong, Ruiqi; Darrell, Trevor; Steinhardt, Jacob; Gonzalez, Joseph E.; Yeung-Levy, Serena","Describing Differences in Image Sets with Natural Language","","","","10.48550/arXiv.2312.02974","http://arxiv.org/abs/2312.02974","How do two sets of images differ? Discerning set-level differences is crucial for understanding model behaviors and analyzing datasets, yet manually sifting through thousands of images is impractical. To aid in this discovery process, we explore the task of automatically describing the differences between two $\textbf{sets}$ of images, which we term Set Difference Captioning. This task takes in image sets $D_A$ and $D_B$, and outputs a description that is more often true on $D_A$ than $D_B$. We outline a two-stage approach that first proposes candidate difference descriptions from image sets and then re-ranks the candidates by checking how well they can differentiate the two sets. We introduce VisDiff, which first captions the images and prompts a language model to propose candidate descriptions, then re-ranks these descriptions using CLIP. To evaluate VisDiff, we collect VisDiffBench, a dataset with 187 paired image sets with ground truth difference descriptions. We apply VisDiff to various domains, such as comparing datasets (e.g., ImageNet vs. ImageNetV2), comparing classification models (e.g., zero-shot CLIP vs. supervised ResNet), summarizing model failure modes (supervised ResNet), characterizing differences between generative models (e.g., StableDiffusionV1 and V2), and discovering what makes images memorable. Using VisDiff, we are able to find interesting and previously unknown differences in datasets and models, demonstrating its utility in revealing nuanced insights.","2024-04-26","2025-04-11 01:51:54","2025-04-11 01:51:54","2025-04-11 01:51:54","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2312.02974 [cs]","","/Users/alihaider/Zotero/storage/YUM8H7EH/Dunlap et al. - 2024 - Describing Differences in Image Sets with Natural Language.pdf; /Users/alihaider/Zotero/storage/N8CL972J/2312.html","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Computation and Language; Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","arXiv:2312.02974","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QQ65RDYI","journalArticle","2025","Gavrikov, Paul; Lukasik, Jovita; Jung, Steffen; Geirhos, Robert; Mirza, M Jehanzeb; Keuper, Margret; Keuper, Janis","CAN WE TALK MODELS INTO SEEING THE WORLD DIFFERENTLY?","","","","","","Unlike traditional vision-only models, vision language models (VLMs) offer an intuitive way to access visual content through language prompting by combining a large language model (LLM) with a vision encoder. However, both the LLM and the vision encoder come with their own set of biases, cue preferences, and shortcuts, which have been rigorously studied in uni-modal models. A timely question is how such (potentially misaligned) biases and cue preferences behave under multi-modal fusion in VLMs. As a first step towards a better understanding, we investigate a particularly well-studied vision-only bias - the texture vs. shape bias and the dominance of local over global information. As expected, we find that VLMs inherit this bias to some extent from their vision encoders. Surprisingly, the multi-modality alone proves to have important effects on the model behavior, i.e., the joint training and the language querying change the way visual cues are processed. While this direct impact of language-informed training on a model’s visual perception is intriguing, it raises further questions on our ability to actively steer a model’s output so that its prediction is based on particular visual cues of the user’s choice. Interestingly, VLMs have an inherent tendency to recognize objects based on shape information, which is different from what a plain vision encoder would do. Further active steering towards shape-based classifications through language prompts is however limited. In contrast, active VLM steering towards texture-based decisions through simple natural language prompts is often more successful.","2025","2025-04-11 02:25:37","2025-04-11 02:25:37","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/alihaider/Zotero/storage/GDNX2JTV/Gavrikov et al. - 2025 - CAN WE TALK MODELS INTO SEEING THE WORLD DIFFERENTLY.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PBXX9MEC","webpage","","","BradyFU/Awesome-Multimodal-Large-Language-Models: :sparkles::sparkles:Latest Advances on Multimodal Large Language Models","","","","","https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models?tab=readme-ov-file#llm-aided-visual-reasoning","","","2025-04-11 02:26:03","2025-04-11 02:26:03","2025-04-11 02:26:03","","","","","","","","","","","","","","","","","","","","","","","/Users/alihaider/Zotero/storage/Z2XVIRWV/Awesome-Multimodal-Large-Language-Models.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZL4IN9Z6","computerProgram","2025","jingyi0000","jingyi0000/VLM_survey","","","","","https://github.com/jingyi0000/VLM_survey","Collection of AWESOME vision-language models for vision tasks","2025-04-10","2025-04-11 02:26:33","2025-04-11 02:26:33","2025-04-11 02:26:33","","","","","","","","","","","","","","","","","","","GitHub","","original-date: 2023-03-30T06:06:59Z","","","","","clip; computer-vision; deep-learning; knowledge-distillation; multi-modal-model; survey; transfer-learning; vision-language-model","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PK5ZH4HQ","preprint","2025","Bolya, Daniel; Huang, Po-Yao; Sun, Peize; Cho, Jang Hyun; Madotto, Andrea; Wei, Chen; Ma, Tengyu; Zhi, Jiale; Rajasegaran, Jathushan; Rasheed, Hanoona; Wang, Junke; Monteiro, Marco; Xu, Hu; Dong, Shiyu; Ravi, Nikhila; Li, Daniel; Dollár, Piotr; Feichtenhofer, Christoph","Perception Encoder: The best visual embeddings are not at the output of the network","","","","10.48550/arXiv.2504.13181","http://arxiv.org/abs/2504.13181","We introduce Perception Encoder (PE), a state-of-the-art encoder for image and video understanding trained via simple vision-language learning. Traditionally, vision encoders have relied on a variety of pretraining objectives, each tailored to specific downstream tasks such as classification, captioning, or localization. Surprisingly, after scaling our carefully tuned image pretraining recipe and refining with our robust video data engine, we find that contrastive vision-language training alone can produce strong, general embeddings for all of these downstream tasks. There is only one caveat: these embeddings are hidden within the intermediate layers of the network. To draw them out, we introduce two alignment methods, language alignment for multimodal language modeling, and spatial alignment for dense prediction. Together with the core contrastive checkpoint, our PE family of models achieves state-of-the-art performance on a wide variety of tasks, including zero-shot image and video classification and retrieval; document, image, and video Q&A; and spatial tasks such as detection, depth estimation, and tracking. To foster further research, we are releasing our models, code, and a novel dataset of synthetically and human-annotated videos.","2025-04-17","2025-04-21 04:33:12","2025-04-21 04:33:12","2025-04-21 04:33:12","","","","","","","Perception Encoder","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2504.13181 [cs]","","/Users/alihaider/Zotero/storage/TQENS2JI/Bolya et al. - 2025 - Perception Encoder The best visual embeddings are not at the output of the network.pdf; /Users/alihaider/Zotero/storage/DC22CSIE/2504.html","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2504.13181","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZR4Z6IJH","preprint","2024","Luo, Ziwei; Gustafsson, Fredrik K.; Zhao, Zheng; Sjölund, Jens; Schön, Thomas B.","Photo-Realistic Image Restoration in the Wild with Controlled Vision-Language Models","","","","10.48550/arXiv.2404.09732","http://arxiv.org/abs/2404.09732","Though diffusion models have been successfully applied to various image restoration (IR) tasks, their performance is sensitive to the choice of training datasets. Typically, diffusion models trained in specific datasets fail to recover images that have out-of-distribution degradations. To address this problem, this work leverages a capable vision-language model and a synthetic degradation pipeline to learn image restoration in the wild (wild IR). More specifically, all low-quality images are simulated with a synthetic degradation pipeline that contains multiple common degradations such as blur, resize, noise, and JPEG compression. Then we introduce robust training for a degradation-aware CLIP model to extract enriched image content features to assist high-quality image restoration. Our base diffusion model is the image restoration SDE (IR-SDE). Built upon it, we further present a posterior sampling strategy for fast noise-free image generation. We evaluate our model on both synthetic and real-world degradation datasets. Moreover, experiments on the unified image restoration task illustrate that the proposed posterior sampling improves image generation quality for various degradations.","2024-04-15","2025-04-25 08:04:36","2025-04-25 08:04:36","2025-04-25 08:04:36","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2404.09732 [cs]","","/Users/alihaider/Zotero/storage/DCIGSV5V/Luo et al. - 2024 - Photo-Realistic Image Restoration in the Wild with Controlled Vision-Language Models.pdf; /Users/alihaider/Zotero/storage/GJSHYJM5/2404.html","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2404.09732","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G7M887S2","preprint","2024","Springer, Jacob Mitchell; Kotha, Suhas; Fried, Daniel; Neubig, Graham; Raghunathan, Aditi","Repetition Improves Language Model Embeddings","","","","10.48550/arXiv.2402.15449","http://arxiv.org/abs/2402.15449","Recent approaches to improving the extraction of text embeddings from autoregressive large language models (LLMs) have largely focused on improvements to data, backbone pretrained language models, or improving task-differentiation via instructions. In this work, we address an architectural limitation of autoregressive models: token embeddings cannot contain information from tokens that appear later in the input. To address this limitation, we propose a simple approach, ""echo embeddings,"" in which we repeat the input twice in context and extract embeddings from the second occurrence. We show that echo embeddings of early tokens can encode information about later tokens, allowing us to maximally leverage high-quality LLMs for embeddings. On the MTEB leaderboard, echo embeddings improve over classical embeddings by over 9% zero-shot and by around 0.7% when fine-tuned. Echo embeddings with a Mistral-7B model achieve state-of-the-art compared to prior open source models that do not leverage synthetic fine-tuning data.","2024-02-23","2025-04-29 03:34:05","2025-04-29 03:34:11","2025-04-29 03:34:05","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2402.15449 [cs]","","/Users/alihaider/Zotero/storage/22XEBTUN/Springer et al. - 2024 - Repetition Improves Language Model Embeddings.pdf; /Users/alihaider/Zotero/storage/G5D9XG3Z/2402.html","","","Computer Science - Machine Learning; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2402.15449","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y8Y5V6EM","preprint","2024","Tong, Bo; Lai, Bokai; Zhou, Yiyi; Luo, Gen; Shen, Yunhang; Li, Ke; Sun, Xiaoshuai; Ji, Rongrong","FlashSloth: Lightning Multimodal Large Language Models via Embedded Visual Compression","","","","10.48550/arXiv.2412.04317","http://arxiv.org/abs/2412.04317","Despite a big leap forward in capability, multimodal large language models (MLLMs) tend to behave like a sloth in practical use, i.e., slow response and large latency. Recent efforts are devoted to building tiny MLLMs for better efficiency, but the plethora of visual tokens still used limit their actual speedup. In this paper, we propose a powerful and fast tiny MLLM called FlashSloth. Different from previous efforts, FlashSloth focuses on improving the descriptive power of visual tokens in the process of compressing their redundant semantics. In particular, FlashSloth introduces embedded visual compression designs to capture both visually salient and instruction-related image information, so as to achieving superior multimodal performance with fewer visual tokens. Extensive experiments are conducted to validate the proposed FlashSloth, and a bunch of tiny but strong MLLMs are also comprehensively compared, e.g., InternVL2, MiniCPM-V2 and Qwen2-VL. The experimental results show that compared with these advanced tiny MLLMs, our FlashSloth can greatly reduce the number of visual tokens, training memory and computation complexity while retaining high performance on various VL tasks.","2024-12-05","2025-05-20 07:04:35","2025-05-20 07:04:39","2025-05-20 07:04:35","","","","","","","FlashSloth","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2412.04317 [cs]","","/Users/alihaider/Zotero/storage/CWGHNWDX/Tong et al. - 2024 - FlashSloth Lightning Multimodal Large Language Models via Embedded Visual Compression.pdf; /Users/alihaider/Zotero/storage/XFW4C7WT/2412.html","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2412.04317","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IG6T7SKA","preprint","2025","Ma, Jun; Yang, Zongxin; Kim, Sumin; Chen, Bihui; Baharoon, Mohammed; Fallahpour, Adibvafa; Asakereh, Reza; Lyu, Hongwei; Wang, Bo","MedSAM2: Segment Anything in 3D Medical Images and Videos","","","","10.48550/arXiv.2504.03600","http://arxiv.org/abs/2504.03600","Medical image and video segmentation is a critical task for precision medicine, which has witnessed considerable progress in developing task or modality-specific and generalist models for 2D images. However, there have been limited studies on building general-purpose models for 3D images and videos with comprehensive user studies. Here, we present MedSAM2, a promptable segmentation foundation model for 3D image and video segmentation. The model is developed by fine-tuning the Segment Anything Model 2 on a large medical dataset with over 455,000 3D image-mask pairs and 76,000 frames, outperforming previous models across a wide range of organs, lesions, and imaging modalities. Furthermore, we implement a human-in-the-loop pipeline to facilitate the creation of large-scale datasets resulting in, to the best of our knowledge, the most extensive user study to date, involving the annotation of 5,000 CT lesions, 3,984 liver MRI lesions, and 251,550 echocardiogram video frames, demonstrating that MedSAM2 can reduce manual costs by more than 85%. MedSAM2 is also integrated into widely used platforms with user-friendly interfaces for local and cloud deployment, making it a practical tool for supporting efficient, scalable, and high-quality segmentation in both research and healthcare environments.","2025-04-04","2025-05-27 04:48:17","2025-05-27 04:48:21","2025-05-27 04:48:17","","","","","","","MedSAM2","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2504.03600 [eess]","","/Users/alihaider/Zotero/storage/3BTBVNZP/Ma et al. - 2025 - MedSAM2 Segment Anything in 3D Medical Images and Videos.pdf; /Users/alihaider/Zotero/storage/K8QL9LV9/2504.html","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Electrical Engineering and Systems Science - Image and Video Processing","","","","","","","","","","","","","","","","","","","arXiv:2504.03600","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VZ9FWVJV","preprint","2024","Qu, Liao; Zhang, Huichao; Liu, Yiheng; Wang, Xu; Jiang, Yi; Gao, Yiming; Ye, Hu; Du, Daniel K.; Yuan, Zehuan; Wu, Xinglong","TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation","","","","10.48550/arXiv.2412.03069","http://arxiv.org/abs/2412.03069","We present TokenFlow, a novel unified image tokenizer that bridges the long-standing gap between multimodal understanding and generation. Prior research attempt to employ a single reconstruction-targeted Vector Quantization (VQ) encoder for unifying these two tasks. We observe that understanding and generation require fundamentally different granularities of visual information. This leads to a critical trade-off, particularly compromising performance in multimodal understanding tasks. TokenFlow addresses this challenge through an innovative dual-codebook architecture that decouples semantic and pixel-level feature learning while maintaining their alignment via a shared mapping mechanism. This design enables direct access to both high-level semantic representations crucial for understanding tasks and fine-grained visual features essential for generation through shared indices. Our extensive experiments demonstrate TokenFlow's superiority across multiple dimensions. Leveraging TokenFlow, we demonstrate for the first time that discrete visual input can surpass LLaVA-1.5 13B in understanding performance, achieving a 7.2\% average improvement. For image reconstruction, we achieve a strong FID score of 0.63 at 384*384 resolution. Moreover, TokenFlow establishes state-of-the-art performance in autoregressive image generation with a GenEval score of 0.55 at 256*256 resolution, achieving comparable results to SDXL.","2024-12-04","2025-05-29 05:54:39","2025-05-29 05:54:44","2025-05-29 05:54:39","","","","","","","TokenFlow","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2412.03069 [cs]","","/Users/alihaider/Zotero/storage/LTESZ4KZ/Qu et al. - 2024 - TokenFlow Unified Image Tokenizer for Multimodal Understanding and Generation.pdf; /Users/alihaider/Zotero/storage/GHTJ9VE3/2412.html","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2412.03069","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IZVB8SSR","journalArticle","2024","Zhang, Jingyi; Huang, Jiaxing; Jin, Sheng; Lu, Shijian","Vision-Language Models for Vision Tasks: A Survey","IEEE Transactions on Pattern Analysis and Machine Intelligence","","1939-3539","10.1109/TPAMI.2024.3369699","https://ieeexplore.ieee.org/document/10445007/","Most visual recognition studies rely heavily on crowd-labelled data in deep neural networks (DNNs) training, and they usually train a DNN for each single visual recognition task, leading to a laborious and time-consuming visual recognition paradigm. To address the two challenges, Vision-Language Models (VLMs) have been intensively investigated recently, which learns rich vision-language correlation from web-scale image-text pairs that are almost infinitely available on the Internet and enables zero-shot predictions on various visual recognition tasks with a single VLM. This paper provides a systematic review of visual language models for various visual recognition tasks, including: (1) the background that introduces the development of visual recognition paradigms; (2) the foundations of VLM that summarize the widely-adopted network architectures, pre-training objectives, and downstream tasks; (3) the widely-adopted datasets in VLM pre-training and evaluations; (4) the review and categorization of existing VLM pre-training methods, VLM transfer learning methods, and VLM knowledge distillation methods; (5) the benchmarking, analysis and discussion of the reviewed methods; (6) several research challenges and potential research directions that could be pursued in the future VLM studies for visual recognition.","2024-08","2025-06-04 05:50:38","2025-06-04 05:50:45","2025-06-04 05:50:38","5625-5644","","8","46","","","Vision-Language Models for Vision Tasks","","","","","","","","","","","","IEEE Xplore","","","","/Users/alihaider/Zotero/storage/M5P5AU55/Zhang et al. - 2024 - Vision-Language Models for Vision Tasks A Survey.pdf","","","deep learning; Deep learning; Big Data; big model; Data models; deep neural network; image classification; knowledge distillation; object detection; pre-training; Predictive models; semantic segmentation; Surveys; Task analysis; Training; transfer learning; vision-language model; visual recognition; Visualization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MHABWSU8","preprint","2021","Radford, Alec; Kim, Jong Wook; Hallacy, Chris; Ramesh, Aditya; Goh, Gabriel; Agarwal, Sandhini; Sastry, Girish; Askell, Amanda; Mishkin, Pamela; Clark, Jack; Krueger, Gretchen; Sutskever, Ilya","Learning Transferable Visual Models From Natural Language Supervision","","","","10.48550/arXiv.2103.00020","http://arxiv.org/abs/2103.00020","State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.","2021-02-26","2025-06-04 05:58:07","2025-06-04 05:58:07","2025-06-04 05:58:07","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2103.00020 [cs]","","/Users/alihaider/Zotero/storage/NF7K4WEM/Radford et al. - 2021 - Learning Transferable Visual Models From Natural Language Supervision.pdf; /Users/alihaider/Zotero/storage/ML3WA399/2103.html","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2103.00020","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4YRSBI2P","preprint","2024","Yang, Senqiao; Chen, Yukang; Tian, Zhuotao; Wang, Chengyao; Li, Jingyao; Yu, Bei; Jia, Jiaya","VisionZip: Longer is Better but Not Necessary in Vision Language Models","","","","10.48550/arXiv.2412.04467","http://arxiv.org/abs/2412.04467","Recent advancements in vision-language models have enhanced performance by increasing the length of visual tokens, making them much longer than text tokens and significantly raising computational costs. However, we observe that the visual tokens generated by popular vision encoders, such as CLIP and SigLIP, contain significant redundancy. To address this, we introduce VisionZip, a simple yet effective method that selects a set of informative tokens for input to the language model, reducing visual token redundancy and improving efficiency while maintaining model performance. The proposed VisionZip can be widely applied to image and video understanding tasks and is well-suited for multi-turn dialogues in real-world scenarios, where previous methods tend to underperform. Experimental results show that VisionZip outperforms the previous state-of-the-art method by at least 5% performance gains across nearly all settings. Moreover, our method significantly enhances model inference speed, improving the prefilling time by 8x and enabling the LLaVA-Next 13B model to infer faster than the LLaVA-Next 7B model while achieving better results. Furthermore, we analyze the causes of this redundancy and encourage the community to focus on extracting better visual features rather than merely increasing token length. Our code is available at https://github.com/dvlab-research/VisionZip .","2024-12-05","2025-06-17 06:55:17","2025-06-17 06:55:24","2025-06-17 06:55:17","","","","","","","VisionZip","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2412.04467 [cs] version: 1","","/Users/alihaider/Zotero/storage/B3CG4C5E/Yang et al. - 2024 - VisionZip Longer is Better but Not Necessary in Vision Language Models.pdf; /Users/alihaider/Zotero/storage/WMDKEABU/2412.html","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2412.04467","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R4VANXUJ","preprint","2025","Vasu, Pavan Kumar Anasosalu; Faghri, Fartash; Li, Chun-Liang; Koc, Cem; True, Nate; Antony, Albert; Santhanam, Gokul; Gabriel, James; Grasch, Peter; Tuzel, Oncel; Pouransari, Hadi","FastVLM: Efficient Vision Encoding for Vision Language Models","","","","10.48550/arXiv.2412.13303","http://arxiv.org/abs/2412.13303","Scaling the input image resolution is essential for enhancing the performance of Vision Language Models (VLMs), particularly in text-rich image understanding tasks. However, popular visual encoders such as ViTs become inefficient at high resolutions due to the large number of tokens and high encoding latency caused by stacked self-attention layers. At different operational resolutions, the vision encoder of a VLM can be optimized along two axes: reducing encoding latency and minimizing the number of visual tokens passed to the LLM, thereby lowering overall latency. Based on a comprehensive efficiency analysis of the interplay between image resolution, vision latency, token count, and LLM size, we introduce FastVLM, a model that achieves an optimized trade-off between latency, model size and accuracy. FastVLM incorporates FastViTHD, a novel hybrid vision encoder designed to output fewer tokens and significantly reduce encoding time for high-resolution images. Unlike previous methods, FastVLM achieves the optimal balance between visual token count and image resolution solely by scaling the input image, eliminating the need for additional token pruning and simplifying the model design. In the LLaVA-1.5 setup, FastVLM achieves 3.2$\times$ improvement in time-to-first-token (TTFT) while maintaining similar performance on VLM benchmarks compared to prior works. Compared to LLaVa-OneVision at the highest resolution (1152$\times$1152), FastVLM achieves better performance on key benchmarks like SeedBench, MMMU and DocVQA, using the same 0.5B LLM, but with 85$\times$ faster TTFT and a vision encoder that is 3.4$\times$ smaller. Code and models are available at https://github.com/apple/ml-fastvlm.","2025-05-15","2025-06-18 03:36:22","2025-06-18 03:36:26","2025-06-18 03:36:22","","","","","","","FastVLM","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2412.13303 [cs]","","/Users/alihaider/Zotero/storage/MAFAPS2P/Vasu et al. - 2025 - FastVLM Efficient Vision Encoding for Vision Language Models.pdf; /Users/alihaider/Zotero/storage/2Z54TYPR/2412.html","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2412.13303","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U8ZYIZTT","preprint","2025","Vasu, Pavan Kumar Anasosalu; Faghri, Fartash; Li, Chun-Liang; Koc, Cem; True, Nate; Antony, Albert; Santhanam, Gokul; Gabriel, James; Grasch, Peter; Tuzel, Oncel; Pouransari, Hadi","FastVLM: Efficient Vision Encoding for Vision Language Models","","","","10.48550/arXiv.2412.13303","http://arxiv.org/abs/2412.13303","Scaling the input image resolution is essential for enhancing the performance of Vision Language Models (VLMs), particularly in text-rich image understanding tasks. However, popular visual encoders such as ViTs become inefficient at high resolutions due to the large number of tokens and high encoding latency caused by stacked self-attention layers. At different operational resolutions, the vision encoder of a VLM can be optimized along two axes: reducing encoding latency and minimizing the number of visual tokens passed to the LLM, thereby lowering overall latency. Based on a comprehensive efficiency analysis of the interplay between image resolution, vision latency, token count, and LLM size, we introduce FastVLM, a model that achieves an optimized trade-off between latency, model size and accuracy. FastVLM incorporates FastViTHD, a novel hybrid vision encoder designed to output fewer tokens and significantly reduce encoding time for high-resolution images. Unlike previous methods, FastVLM achieves the optimal balance between visual token count and image resolution solely by scaling the input image, eliminating the need for additional token pruning and simplifying the model design. In the LLaVA-1.5 setup, FastVLM achieves 3.2$\times$ improvement in time-to-first-token (TTFT) while maintaining similar performance on VLM benchmarks compared to prior works. Compared to LLaVa-OneVision at the highest resolution (1152$\times$1152), FastVLM achieves better performance on key benchmarks like SeedBench, MMMU and DocVQA, using the same 0.5B LLM, but with 85$\times$ faster TTFT and a vision encoder that is 3.4$\times$ smaller. Code and models are available at https://github.com/apple/ml-fastvlm.","2025-05-15","2025-06-18 03:39:28","2025-06-18 03:39:28","2025-06-18 03:39:28","","","","","","","FastVLM","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2412.13303 [cs]","","/Users/alihaider/Zotero/storage/EBYF2H8Z/Vasu et al. - 2025 - FastVLM Efficient Vision Encoding for Vision Language Models.pdf; /Users/alihaider/Zotero/storage/ULCNA2XU/2412.html","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2412.13303","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7YMIC9RL","preprint","2024","Chen, Haoyu; Li, Wenbo; Gu, Jinjin; Ren, Jingjing; Chen, Sixiang; Ye, Tian; Pei, Renjing; Zhou, Kaiwen; Song, Fenglong; Zhu, Lei","RestoreAgent: Autonomous Image Restoration Agent via Multimodal Large Language Models","","","","10.48550/arXiv.2407.18035","http://arxiv.org/abs/2407.18035","Natural images captured by mobile devices often suffer from multiple types of degradation, such as noise, blur, and low light. Traditional image restoration methods require manual selection of specific tasks, algorithms, and execution sequences, which is time-consuming and may yield suboptimal results. All-in-one models, though capable of handling multiple tasks, typically support only a limited range and often produce overly smooth, low-fidelity outcomes due to their broad data distribution fitting. To address these challenges, we first define a new pipeline for restoring images with multiple degradations, and then introduce RestoreAgent, an intelligent image restoration system leveraging multimodal large language models. RestoreAgent autonomously assesses the type and extent of degradation in input images and performs restoration through (1) determining the appropriate restoration tasks, (2) optimizing the task sequence, (3) selecting the most suitable models, and (4) executing the restoration. Experimental results demonstrate the superior performance of RestoreAgent in handling complex degradation, surpassing human experts. Furthermore, the system modular design facilitates the fast integration of new tasks and models, enhancing its flexibility and scalability for various applications.","2024-07-25","2025-06-27 09:27:11","2025-06-27 09:27:25","2025-06-27 09:27:11","","","","","","","RestoreAgent","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2407.18035 [cs]","","/Users/alihaider/Zotero/storage/5JWH5MHE/Chen et al. - 2024 - RestoreAgent Autonomous Image Restoration Agent via Multimodal Large Language Models.pdf; /Users/alihaider/Zotero/storage/HD9Q258R/2407.html","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2407.18035","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6A268AYU","preprint","2025","Azad, Shehreen; Jain, Yash; Garg, Rishit; Rawat, Yogesh S.; Vineet, Vibhav","Understanding Depth and Height Perception in Large Visual-Language Models","","","","10.48550/arXiv.2408.11748","http://arxiv.org/abs/2408.11748","Geometric understanding - including depth and height perception - is fundamental to intelligence and crucial for navigating our environment. Despite the impressive capabilities of large Vision Language Models (VLMs), it remains unclear how well they possess the geometric understanding required for practical applications in visual perception. In this work, we focus on evaluating the geometric understanding of these models, specifically targeting their ability to perceive the depth and height of objects in an image. To address this, we introduce GeoMeter, a suite of benchmark datasets - encompassing 2D and 3D scenarios - to rigorously evaluate these aspects. By benchmarking 18 state-of-the-art VLMs, we found that although they excel in perceiving basic geometric properties like shape and size, they consistently struggle with depth and height perception. Our analysis reveal that these challenges stem from shortcomings in their depth and height reasoning capabilities and inherent biases. This study aims to pave the way for developing VLMs with enhanced geometric understanding by emphasizing depth and height perception as critical components necessary for real-world applications.","2025-04-25","2025-06-30 05:16:38","2025-06-30 05:16:38","2025-06-30 05:16:38","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2408.11748 [cs]","","/Users/alihaider/Zotero/storage/248RV8SH/Azad et al. - 2025 - Understanding Depth and Height Perception in Large Visual-Language Models.pdf; /Users/alihaider/Zotero/storage/3LXZ48CZ/2408.html","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2408.11748","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NVJFX3ET","preprint","2025","Schnaus, Dominik; Araslanov, Nikita; Cremers, Daniel","It's a (Blind) Match! Towards Vision-Language Correspondence without Parallel Data","","","","10.48550/arXiv.2503.24129","http://arxiv.org/abs/2503.24129","The platonic representation hypothesis suggests that vision and language embeddings become more homogeneous as model and dataset sizes increase. In particular, pairwise distances within each modality become more similar. This suggests that as foundation models mature, it may become possible to match vision and language embeddings in a fully unsupervised fashion, i.e. without parallel data. We present the first feasibility study, and investigate conformity of existing vision and language foundation models in the context of unsupervised, or ""blind"", matching. First, we formulate unsupervised matching as a quadratic assignment problem and introduce a novel heuristic that outperforms previous solvers. We also develop a technique to find optimal matching problems, for which a non-trivial match is very likely. Second, we conduct an extensive study deploying a range of vision and language models on four datasets. Our analysis reveals that for many problem instances, vision and language representations can be indeed matched without supervision. This finding opens up the exciting possibility of embedding semantic knowledge into other modalities virtually annotation-free. As a proof of concept, we showcase an unsupervised classifier, which achieves non-trivial classification accuracy without any image-text annotation.","2025-05-29","2025-06-30 05:19:05","2025-06-30 05:19:11","2025-06-30 05:19:05","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2503.24129 [cs]","","/Users/alihaider/Zotero/storage/IVP3GSFF/Schnaus et al. - 2025 - It's a (Blind) Match! Towards Vision-Language Correspondence without Parallel Data.pdf; /Users/alihaider/Zotero/storage/RSSAU9NC/2503.html","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2503.24129","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DN7ISVPX","preprint","2025","Upadhyay, Ujjwal; Ranjan, Mukul; Shen, Zhiqiang; Elhoseiny, Mohamed","Time Blindness: Why Video-Language Models Can't See What Humans Can?","","","","10.48550/arXiv.2505.24867","http://arxiv.org/abs/2505.24867","Recent advances in vision-language models (VLMs) have made impressive strides in understanding spatio-temporal relationships in videos. However, when spatial information is obscured, these models struggle to capture purely temporal patterns. We introduce $\textbf{SpookyBench}$, a benchmark where information is encoded solely in temporal sequences of noise-like frames, mirroring natural phenomena from biological signaling to covert communication. Interestingly, while humans can recognize shapes, text, and patterns in these sequences with over 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance gap highlights a critical limitation: an over-reliance on frame-level spatial features and an inability to extract meaning from temporal cues. Furthermore, when trained in data sets with low spatial signal-to-noise ratios (SNR), temporal understanding of models degrades more rapidly than human perception, especially in tasks requiring fine-grained temporal reasoning. Overcoming this limitation will require novel architectures or training paradigms that decouple spatial dependencies from temporal processing. Our systematic analysis shows that this issue persists across model scales and architectures. We release SpookyBench to catalyze research in temporal pattern recognition and bridge the gap between human and machine video understanding. Dataset and code has been made available on our project website: https://timeblindness.github.io/.","2025-05-30","2025-06-30 05:19:40","2025-06-30 05:19:40","2025-06-30 05:19:40","","","","","","","Time Blindness","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2505.24867 [cs]","","/Users/alihaider/Zotero/storage/UTMXP4P9/Upadhyay et al. - 2025 - Time Blindness Why Video-Language Models Can't See What Humans Can.pdf; /Users/alihaider/Zotero/storage/F6ABMSPP/2505.html","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2505.24867","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E9XZQ938","preprint","2025","Assran, Mido; Bardes, Adrien; Fan, David; Garrido, Quentin; Howes, Russell; Mojtaba; Komeili; Muckley, Matthew; Rizvi, Ammar; Roberts, Claire; Sinha, Koustuv; Zholus, Artem; Arnaud, Sergio; Gejji, Abha; Martin, Ada; Hogan, Francois Robert; Dugas, Daniel; Bojanowski, Piotr; Khalidov, Vasil; Labatut, Patrick; Massa, Francisco; Szafraniec, Marc; Krishnakumar, Kapil; Li, Yong; Ma, Xiaodong; Chandar, Sarath; Meier, Franziska; LeCun, Yann; Rabbat, Michael; Ballas, Nicolas","V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning","","","","10.48550/arXiv.2506.09985","http://arxiv.org/abs/2506.09985","A major challenge for modern AI is to learn to understand the world and learn to act largely by observation. This paper explores a self-supervised approach that combines internet-scale video data with a small amount of interaction data (robot trajectories), to develop models capable of understanding, predicting, and planning in the physical world. We first pre-train an action-free joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset comprising over 1 million hours of internet video. V-JEPA 2 achieves strong performance on motion understanding (77.3 top-1 accuracy on Something-Something v2) and state-of-the-art performance on human action anticipation (39.7 recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models. Additionally, after aligning V-JEPA 2 with a large language model, we demonstrate state-of-the-art performance on multiple video question-answering tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on TempCompass). Finally, we show how self-supervised learning can be applied to robotic planning tasks by post-training a latent action-conditioned world model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the Droid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different labs and enable picking and placing of objects using planning with image goals. Notably, this is achieved without collecting any data from the robots in these environments, and without any task-specific training or reward. This work demonstrates how self-supervised learning from web-scale data and a small amount of robot interaction data can yield a world model capable of planning in the physical world.","2025-06-11","2025-06-30 05:25:51","2025-06-30 05:25:55","2025-06-30 05:25:51","","","","","","","V-JEPA 2","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2506.09985 [cs]","","/Users/alihaider/Zotero/storage/UAG8FFSI/Assran et al. - 2025 - V-JEPA 2 Self-Supervised Video Models Enable Understanding, Prediction and Planning.pdf; /Users/alihaider/Zotero/storage/UHYTGGMX/2506.html","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Robotics","","","","","","","","","","","","","","","","","","","arXiv:2506.09985","","","","","","","","","","","","","","","","","","","","","","","","","","",""