"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"R4VANXUJ","preprint","2025","Vasu, Pavan Kumar Anasosalu; Faghri, Fartash; Li, Chun-Liang; Koc, Cem; True, Nate; Antony, Albert; Santhanam, Gokul; Gabriel, James; Grasch, Peter; Tuzel, Oncel; Pouransari, Hadi","FastVLM: Efficient Vision Encoding for Vision Language Models","","","","10.48550/arXiv.2412.13303","http://arxiv.org/abs/2412.13303","Scaling the input image resolution is essential for enhancing the performance of Vision Language Models (VLMs), particularly in text-rich image understanding tasks. However, popular visual encoders such as ViTs become inefficient at high resolutions due to the large number of tokens and high encoding latency caused by stacked self-attention layers. At different operational resolutions, the vision encoder of a VLM can be optimized along two axes: reducing encoding latency and minimizing the number of visual tokens passed to the LLM, thereby lowering overall latency. Based on a comprehensive efficiency analysis of the interplay between image resolution, vision latency, token count, and LLM size, we introduce FastVLM, a model that achieves an optimized trade-off between latency, model size and accuracy. FastVLM incorporates FastViTHD, a novel hybrid vision encoder designed to output fewer tokens and significantly reduce encoding time for high-resolution images. Unlike previous methods, FastVLM achieves the optimal balance between visual token count and image resolution solely by scaling the input image, eliminating the need for additional token pruning and simplifying the model design. In the LLaVA-1.5 setup, FastVLM achieves 3.2$\times$ improvement in time-to-first-token (TTFT) while maintaining similar performance on VLM benchmarks compared to prior works. Compared to LLaVa-OneVision at the highest resolution (1152$\times$1152), FastVLM achieves better performance on key benchmarks like SeedBench, MMMU and DocVQA, using the same 0.5B LLM, but with 85$\times$ faster TTFT and a vision encoder that is 3.4$\times$ smaller. Code and models are available at https://github.com/apple/ml-fastvlm.","2025-05-15","2025-06-18 03:36:22","2025-06-18 03:36:26","2025-06-18 03:36:22","","","","","","","FastVLM","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2412.13303 [cs]","","/Users/alihaider/Zotero/storage/MAFAPS2P/Vasu et al. - 2025 - FastVLM Efficient Vision Encoding for Vision Language Models.pdf; /Users/alihaider/Zotero/storage/2Z54TYPR/2412.html","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2412.13303","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2U2J2GIP","preprint","2025","Kutscher, Declan; Chan, David M.; Bai, Yutong; Darrell, Trevor; Gupta, Ritwik","REOrdering Patches Improves Vision Models","","","","10.48550/arXiv.2505.23751","http://arxiv.org/abs/2505.23751","Sequence models such as transformers require inputs to be represented as one-dimensional sequences. In vision, this typically involves flattening images using a fixed row-major (raster-scan) order. While full self-attention is permutation-equivariant, modern long-sequence transformers increasingly rely on architectural approximations that break this invariance and introduce sensitivity to patch ordering. We show that patch order significantly affects model performance in such settings, with simple alternatives like column-major or Hilbert curves yielding notable accuracy shifts. Motivated by this, we propose REOrder, a two-stage framework for discovering task-optimal patch orderings. First, we derive an information-theoretic prior by evaluating the compressibility of various patch sequences. Then, we learn a policy over permutations by optimizing a Plackett-Luce policy using REINFORCE. This approach enables efficient learning in a combinatorial permutation space. REOrder improves top-1 accuracy over row-major ordering on ImageNet-1K by up to 3.01% and Functional Map of the World by 13.35%.","2025-05-29","2025-06-25 03:38:43","2025-06-25 03:39:04","2025-06-25 03:38:43","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2505.23751 [cs]","","/Users/alihaider/Zotero/storage/23S3GKJC/Kutscher et al. - 2025 - REOrdering Patches Improves Vision Models.pdf; /Users/alihaider/Zotero/storage/V4YQZEJG/2505.html","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2505.23751","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4A5UBKWY","preprint","2024","Yoa, Seungdong; Lee, Seungjun; Cho, Hyeseung; Kim, Bumsoo; Lim, Woohyung","ImagePiece: Content-aware Re-tokenization for Efficient Image Recognition","","","","10.48550/arXiv.2412.16491","http://arxiv.org/abs/2412.16491","Vision Transformers (ViTs) have achieved remarkable success in various computer vision tasks. However, ViTs have a huge computational cost due to their inherent reliance on multi-head self-attention (MHSA), prompting efforts to accelerate ViTs for practical applications. To this end, recent works aim to reduce the number of tokens, mainly focusing on how to effectively prune or merge them. Nevertheless, since ViT tokens are generated from non-overlapping grid patches, they usually do not convey sufficient semantics, making it incompatible with efficient ViTs. To address this, we propose ImagePiece, a novel re-tokenization strategy for Vision Transformers. Following the MaxMatch strategy of NLP tokenization, ImagePiece groups semantically insufficient yet locally coherent tokens until they convey meaning. This simple retokenization is highly compatible with previous token reduction methods, being able to drastically narrow down relevant tokens, enhancing the inference speed of DeiT-S by 54% (nearly 1.5$\times$ faster) while achieving a 0.39% improvement in ImageNet classification accuracy. For hyper-speed inference scenarios (with 251% acceleration), our approach surpasses other baselines by an accuracy over 8%.","2024-12-21","2025-06-25 03:45:13","2025-06-25 03:45:13","2025-06-25 03:45:13","","","","","","","ImagePiece","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2412.16491 [cs] version: 1","","/Users/alihaider/Zotero/storage/DWDIVIEY/Yoa et al. - 2024 - ImagePiece Content-aware Re-tokenization for Efficient Image Recognition.pdf; /Users/alihaider/Zotero/storage/GRP4RYKX/2412.html","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2412.16491","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BQIC4YC4","preprint","2024","Yoa, Seungdong; Lee, Seungjun; Cho, Hyeseung; Kim, Bumsoo; Lim, Woohyung","ImagePiece: Content-aware Re-tokenization for Efficient Image Recognition","","","","10.48550/arXiv.2412.16491","http://arxiv.org/abs/2412.16491","Vision Transformers (ViTs) have achieved remarkable success in various computer vision tasks. However, ViTs have a huge computational cost due to their inherent reliance on multi-head self-attention (MHSA), prompting efforts to accelerate ViTs for practical applications. To this end, recent works aim to reduce the number of tokens, mainly focusing on how to effectively prune or merge them. Nevertheless, since ViT tokens are generated from non-overlapping grid patches, they usually do not convey sufficient semantics, making it incompatible with efficient ViTs. To address this, we propose ImagePiece, a novel re-tokenization strategy for Vision Transformers. Following the MaxMatch strategy of NLP tokenization, ImagePiece groups semantically insufficient yet locally coherent tokens until they convey meaning. This simple retokenization is highly compatible with previous token reduction methods, being able to drastically narrow down relevant tokens, enhancing the inference speed of DeiT-S by 54% (nearly 1.5$\times$ faster) while achieving a 0.39% improvement in ImageNet classification accuracy. For hyper-speed inference scenarios (with 251% acceleration), our approach surpasses other baselines by an accuracy over 8%.","2024-12-21","2025-06-30 05:16:55","2025-06-30 05:16:59","2025-06-30 05:16:55","","","","","","","ImagePiece","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2412.16491 [cs] version: 1","","/Users/alihaider/Zotero/storage/58GEXHZT/Yoa et al. - 2024 - ImagePiece Content-aware Re-tokenization for Efficient Image Recognition.pdf; /Users/alihaider/Zotero/storage/MWTQ4Y5Z/2412.html","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2412.16491","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JYXHZ7BC","preprint","2025","Kutscher, Declan; Chan, David M.; Bai, Yutong; Darrell, Trevor; Gupta, Ritwik","REOrdering Patches Improves Vision Models","","","","10.48550/arXiv.2505.23751","http://arxiv.org/abs/2505.23751","Sequence models such as transformers require inputs to be represented as one-dimensional sequences. In vision, this typically involves flattening images using a fixed row-major (raster-scan) order. While full self-attention is permutation-equivariant, modern long-sequence transformers increasingly rely on architectural approximations that break this invariance and introduce sensitivity to patch ordering. We show that patch order significantly affects model performance in such settings, with simple alternatives like column-major or Hilbert curves yielding notable accuracy shifts. Motivated by this, we propose REOrder, a two-stage framework for discovering task-optimal patch orderings. First, we derive an information-theoretic prior by evaluating the compressibility of various patch sequences. Then, we learn a policy over permutations by optimizing a Plackett-Luce policy using REINFORCE. This approach enables efficient learning in a combinatorial permutation space. REOrder improves top-1 accuracy over row-major ordering on ImageNet-1K by up to 3.01% and Functional Map of the World by 13.35%.","2025-05-29","2025-06-30 05:24:39","2025-06-30 05:24:43","2025-06-30 05:24:39","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2505.23751 [cs] version: 1","","/Users/alihaider/Zotero/storage/4LQUNX2L/Kutscher et al. - 2025 - REOrdering Patches Improves Vision Models.pdf; /Users/alihaider/Zotero/storage/F8YE62W3/2505.html","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2505.23751","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QND3RIWQ","preprint","2024","Bergner, Benjamin; Lippert, Christoph; Mahendran, Aravindh","Token Cropr: Faster ViTs for Quite a Few Tasks","","","","10.48550/arXiv.2412.00965","http://arxiv.org/abs/2412.00965","The adoption of Vision Transformers (ViTs) in resource-constrained applications necessitates improvements in inference throughput. To this end several token pruning and merging approaches have been proposed that improve efficiency by successively reducing the number of tokens. However, it remains an open problem to design a token reduction method that is fast, maintains high performance, and is applicable to various vision tasks. In this work, we present a token pruner that uses auxiliary prediction heads that learn to select tokens end-to-end based on task relevance. These auxiliary heads can be removed after training, leading to throughput close to that of a random pruner. We evaluate our method on image classification, semantic segmentation, object detection, and instance segmentation, and show speedups of 1.5 to 4x with small drops in performance. As a best case, on the ADE20k semantic segmentation benchmark, we observe a 2x speedup relative to the no-pruning baseline, with a negligible performance penalty of 0.1 median mIoU across 5 seeds.","2024-12-01","2025-06-30 05:25:19","2025-06-30 05:25:19","2025-06-30 05:25:19","","","","","","","Token Cropr","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2412.00965 [cs] version: 1","","/Users/alihaider/Zotero/storage/QSTLB4D9/Bergner et al. - 2024 - Token Cropr Faster ViTs for Quite a Few Tasks.pdf; /Users/alihaider/Zotero/storage/NMU7E5PM/2412.html","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2412.00965","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZSLFUCKD","preprint","2025","Zeng, Fanhu; Yu, Deli; Kong, Zhenglun; Tang, Hao","Token Transforming: A Unified and Training-Free Token Compression Framework for Vision Transformer Acceleration","","","","10.48550/arXiv.2506.05709","http://arxiv.org/abs/2506.05709","Vision transformers have been widely explored in various vision tasks. Due to heavy computational cost, much interest has aroused for compressing vision transformer dynamically in the aspect of tokens. Current methods mainly pay attention to token pruning or merging to reduce token numbers, in which tokens are compressed exclusively, causing great information loss and therefore post-training is inevitably required to recover the performance. In this paper, we rethink token reduction and unify the process as an explicit form of token matrix transformation, in which all existing methods are constructing special forms of matrices within the framework. Furthermore, we propose a many-to-many Token Transforming framework that serves as a generalization of all existing methods and reserves the most information, even enabling training-free acceleration. We conduct extensive experiments to validate our framework. Specifically, we reduce 40% FLOPs and accelerate DeiT-S by $\times$1.5 with marginal 0.1% accuracy drop. Furthermore, we extend the method to dense prediction tasks including segmentation, object detection, depth estimation, and language model generation. Results demonstrate that the proposed method consistently achieves substantial improvements, offering a better computation-performance trade-off, impressive budget reduction and inference acceleration.","2025-06-06","2025-06-30 05:50:50","2025-06-30 05:50:52","2025-06-30 05:50:50","","","","","","","Token Transforming","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2506.05709 [cs]","","/Users/alihaider/Zotero/storage/KRBAIFGD/Zeng et al. - 2025 - Token Transforming A Unified and Training-Free Token Compression Framework for Vision Transformer A.pdf; /Users/alihaider/Zotero/storage/9VPQRQ5F/2506.html","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2506.05709","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FL39TW35","preprint","2025","Jeddi, Ahmadreza; Baghbanzadeh, Negin; Dolatabadi, Elham; Taati, Babak","Similarity-Aware Token Pruning: Your VLM but Faster","","","","10.48550/arXiv.2503.11549","http://arxiv.org/abs/2503.11549","The computational demands of Vision Transformers (ViTs) and Vision-Language Models (VLMs) remain a significant challenge due to the quadratic complexity of self-attention. While token pruning offers a promising solution, existing methods often introduce training overhead or fail to adapt dynamically across layers. We present SAINT, a training-free token pruning framework that leverages token similarity and a graph-based formulation to dynamically optimize pruning rates and redundancy thresholds. Through systematic analysis, we identify a universal three-stage token evolution process (aligner-explorer-aggregator) in transformers, enabling aggressive pruning in early stages without sacrificing critical information. For ViTs, SAINT doubles the throughput of ViT-H/14 at 224px with only 0.6% accuracy loss on ImageNet-1K, surpassing the closest competitor by 0.8%. For VLMs, we apply SAINT in three modes: ViT-only, LLM-only, and hybrid. SAINT reduces LLaVA-13B's tokens by 75%, achieving latency comparable to LLaVA-7B with less than 1% performance loss across benchmarks. Our work establishes a unified, practical framework for efficient inference in ViTs and VLMs.","2025-03-14","2025-06-30 05:54:15","2025-06-30 05:54:15","2025-06-30 05:54:15","","","","","","","Similarity-Aware Token Pruning","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2503.11549 [cs]","","/Users/alihaider/Zotero/storage/257DKXQQ/Jeddi et al. - 2025 - Similarity-Aware Token Pruning Your VLM but Faster.pdf; /Users/alihaider/Zotero/storage/CA687CEF/2503.html","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2503.11549","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9VAHM6UJ","preprint","2025","Yang, Cheng; Sui, Yang; Xiao, Jinqi; Huang, Lingyi; Gong, Yu; Li, Chendi; Yan, Jinghua; Bai, Yu; Sadayappan, Ponnuswamy; Hu, Xia; Yuan, Bo","TopV: Compatible Token Pruning with Inference Time Optimization for Fast and Low-Memory Multimodal Vision Language Model","","","","10.48550/arXiv.2503.18278","http://arxiv.org/abs/2503.18278","Vision-Language Models (VLMs) demand substantial computational resources during inference, largely due to the extensive visual input tokens for representing visual information. Previous studies have noted that visual tokens tend to receive less attention than text tokens, suggesting their lower importance during inference and potential for pruning. However, their methods encounter several challenges: reliance on greedy heuristic criteria for token importance and incompatibility with FlashAttention and KV cache. To address these issues, we introduce \textbf{TopV}, a compatible \textbf{TO}ken \textbf{P}runing with inference Time Optimization for fast and low-memory \textbf{V}LM, achieving efficient pruning without additional training or fine-tuning. Instead of relying on attention scores, we formulate token pruning as an optimization problem, accurately identifying important visual tokens while remaining compatible with FlashAttention. Additionally, since we only perform this pruning once during the prefilling stage, it effectively reduces KV cache size. Our optimization framework incorporates a visual-aware cost function considering factors such as Feature Similarity, Relative Spatial Distance, and Absolute Central Distance, to measure the importance of each source visual token, enabling effective pruning of low-importance tokens. Extensive experiments demonstrate that our method outperforms previous token pruning methods, validating the effectiveness and efficiency of our approach.","2025-03-29","2025-06-30 05:54:56","2025-06-30 05:54:56","2025-06-30 05:54:56","","","","","","","TopV","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2503.18278 [cs] version: 2","","/Users/alihaider/Zotero/storage/GFH58HZW/Yang et al. - 2025 - TopV Compatible Token Pruning with Inference Time Optimization for Fast and Low-Memory Multimodal V.pdf; /Users/alihaider/Zotero/storage/RBCCKSUT/2503.html","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2503.18278","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QP55T3H9","journalArticle","","Lee, Dong Hoon; Hong, Seunghoon","Learning to Merge Tokens via Decoupled Embedding for Efficient Vision Transformers","","","","","","Recent token reduction methods for Vision Transformers (ViTs) incorporate token merging, which measures the similarities between token embeddings and combines the most similar pairs. However, their merging policies are directly dependent on intermediate features in ViTs, which prevents exploiting features tailored for merging and requires end-to-end training to improve token merging. This paper proposes Decoupled Token Embedding for Merging (DTEM) that enhances token merging through a decoupled embedding learned via a continuously relaxed token merging process. Our method introduces a lightweight embedding module decoupled from the ViT forward pass to extract dedicated features for token merging, addressing the restriction from using intermediate features. The continuously relaxed token merging, applied during training, enables us to learn the decoupled embeddings in a differentiable manner. Thanks to the decoupled structure, our method can be seamlessly integrated into existing ViT backbones and trained either modularly by learning only the decoupled embeddings or end-to-end by fine-tuning. We demonstrate the applicability of DTEM on various tasks, including classification, captioning, and segmentation, with consistent improvement in token merging. Especially in the ImageNet-1k classification, DTEM achieves a 37.2% reduction in FLOPs while maintaining a top-1 accuracy of 79.85% with DeiT-small. Code is available at https://github.com/movinghoon/dtem.","","2025-06-30 06:09:27","2025-06-30 06:09:27","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/alihaider/Zotero/storage/QGSIG8F2/Lee and Hong - Learning to Merge Tokens via Decoupled Embedding for Efficient Vision Transformers.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FVMDCDQH","preprint","2025","Zhang, Ce; Ma, Kaixin; Fang, Tianqing; Yu, Wenhao; Zhang, Hongming; Zhang, Zhisong; Xie, Yaqi; Sycara, Katia; Mi, Haitao; Yu, Dong","VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models","","","","10.48550/arXiv.2505.22654","http://arxiv.org/abs/2505.22654","Recent Large Vision-Language Models (LVLMs) have advanced multi-modal understanding by incorporating finer-grained visual perception and encoding. However, such methods incur significant computational costs due to longer visual token sequences, posing challenges for real-time deployment. To mitigate this, prior studies have explored pruning unimportant visual tokens either at the output layer of the visual encoder or at the early layers of the language model. In this work, we revisit these design choices and reassess their effectiveness through comprehensive empirical studies of how visual tokens are processed throughout the visual encoding and language decoding stages. Guided by these insights, we propose VScan, a two-stage visual token reduction framework that addresses token redundancy by: (1) integrating complementary global and local scans with token merging during visual encoding, and (2) introducing pruning at intermediate layers of the language model. Extensive experimental results across four LVLMs validate the effectiveness of VScan in accelerating inference and demonstrate its superior performance over current state-of-the-arts on sixteen benchmarks. Notably, when applied to LLaVA-NeXT-7B, VScan achieves a 2.91$\times$ speedup in prefilling and a 10$\times$ reduction in FLOPs, while retaining 95.4\% of the original performance. Code is available at https://github.com/Tencent/SelfEvolvingAgent/tree/main/VScan.","2025-06-12","2025-06-30 06:10:09","2025-06-30 06:10:09","2025-06-30 06:10:09","","","","","","","VScan","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2505.22654 [cs] version: 2","","/Users/alihaider/Zotero/storage/2CRLKT58/Zhang et al. - 2025 - VScan Rethinking Visual Token Reduction for Efficient Large Vision-Language Models.pdf; /Users/alihaider/Zotero/storage/CKT5827D/2505.html","","","Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2505.22654","","","","","","","","","","","","","","","","","","","","","","","","","","",""