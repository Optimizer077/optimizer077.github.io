"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"RQMUBPLY","preprint","2023","Zhang, Junyi; Herrmann, Charles; Hur, Junhwa; Cabrera, Luisa Polania; Jampani, Varun; Sun, Deqing; Yang, Ming-Hsuan","A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence","","","","10.48550/arXiv.2305.15347","http://arxiv.org/abs/2305.15347","Text-to-image diffusion models have made significant advances in generating and editing high-quality images. As a result, numerous approaches have explored the ability of diffusion model features to understand and process single images for downstream tasks, e.g., classification, semantic segmentation, and stylization. However, significantly less is known about what these features reveal across multiple, different images and objects. In this work, we exploit Stable Diffusion (SD) features for semantic and dense correspondence and discover that with simple postprocessing, SD features can perform quantitatively similar to SOTA representations. Interestingly, our analysis reveals that SD features have very different properties compared to existing representation learning features, such as the recently released DINOv2: while DINOv2 provides sparse but accurate matches, SD features provide high-quality spatial information but sometimes inaccurate semantic matches. We demonstrate that a simple fusion of the two features works surprisingly well, and a zero-shot evaluation via nearest neighbor search on the fused features provides a significant performance gain over state-of-the-art methods on benchmark datasets, e.g., SPair-71k, PF-Pascal, and TSS. We also show that these correspondences enable high-quality object swapping without task-specific fine-tuning.","2023-11-28","2025-04-11 01:38:07","2025-04-11 01:38:07","2025-04-11 01:38:07","","","","","","","A Tale of Two Features","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2305.15347 [cs]","","/Users/alihaider/Zotero/storage/5ZQZWBUG/Zhang et al. - 2023 - A Tale of Two Features Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2305.15347","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IG2EBFCM","preprint","2025","Yu, Sihyun; Kwak, Sangkyung; Jang, Huiwon; Jeong, Jongheon; Huang, Jonathan; Shin, Jinwoo; Xie, Saining","Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think","","","","10.48550/arXiv.2410.06940","http://arxiv.org/abs/2410.06940","Recent studies have shown that the denoising process in (generative) diffusion models can induce meaningful (discriminative) representations inside the model, though the quality of these representations still lags behind those learned through recent self-supervised learning methods. We argue that one main bottleneck in training large-scale diffusion models for generation lies in effectively learning these representations. Moreover, training can be made easier by incorporating high-quality external visual representations, rather than relying solely on the diffusion models to learn them independently. We study this by introducing a straightforward regularization called REPresentation Alignment (REPA), which aligns the projections of noisy input hidden states in denoising networks with clean image representations obtained from external, pretrained visual encoders. The results are striking: our simple strategy yields significant improvements in both training efficiency and generation quality when applied to popular diffusion and flow-based transformers, such as DiTs and SiTs. For instance, our method can speed up SiT training by over 17.5×, matching the performance (without classifier-free guidance) of a SiT-XL model trained for 7M steps in less than 400K steps. In terms of final generation quality, our approach achieves state-of-the-art results of FID=1.42 using classifier-free guidance with the guidance interval.","2025-02-28","2025-04-11 01:38:29","2025-04-11 01:38:29","2025-04-11 01:38:29","","","","","","","Representation Alignment for Generation","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2410.06940 [cs]","","/Users/alihaider/Zotero/storage/WKV459LW/Jiang_SCEdit_Efficient_and_Controllable_Image_Diffusion_Generation_via_Skip_Connection_CVPR_2024_paper.pdf; /Users/alihaider/Zotero/storage/IPLMN4U7/Yu et al. - 2025 - Representation Alignment for Generation Training Diffusion Transformers Is Easier Than You Think.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2410.06940","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H8JPCUPZ","journalArticle","2025","Chen, Chen; Liu, Daochang; Shah, Mubarak; Xu, Chang","EXPLORING LOCAL MEMORIZATION IN DIFFUSION MODELS VIA BRIGHT ENDING ATTENTION","","","","","","Text-to-image diffusion models have achieved unprecedented proficiency in generating realistic images. However, their inherent tendency to memorize and replicate training data during inference raises significant concerns, including potential copyright infringement. In response, various methods have been proposed to evaluate, detect, and mitigate memorization. Our analysis reveals that existing approaches significantly underperform in handling local memorization, where only specific image regions are memorized, compared to global memorization, where the entire image is replicated. Also, they cannot locate the local memorization regions, making it hard to investigate locally. To address these, we identify a novel “bright ending” (BE) anomaly in diffusion models prone to memorizing training images. BE refers to a distinct cross-attention pattern observed in text-to-image diffusion models, where memorized image patches exhibit significantly greater attention to the final text token during the last inference step than non-memorized patches. This pattern highlights regions where the generated image replicates training data and enables efficient localization of memorized regions. Equipped with this, we propose a simple yet effective method to integrate BE into existing frameworks, significantly improving their performance by narrowing the performance gap caused by local memorization. Our results not only validate the successful execution of the new localization task but also establish new state-of-the-art performance across all existing tasks, underscoring the significance of the BE phenomenon.","2025","2025-04-11 01:46:17","2025-04-11 01:46:17","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/alihaider/Zotero/storage/VRZI6RBC/Chen et al. - 2025 - EXPLORING LOCAL MEMORIZATION IN DIFFUSION MODELS VIA BRIGHT ENDING ATTENTION.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BQK5N5V7","preprint","2025","Xu, Zihao; Tang, Yuzhi; Xu, Bowen; Li, Qingquan","NeurOp-Diff:Continuous Remote Sensing Image Super-Resolution via Neural Operator Diffusion","","","","10.48550/arXiv.2501.09054","http://arxiv.org/abs/2501.09054","Most publicly accessible remote sensing data suffer from low resolution, limiting their practical applications. To address this, we propose a diffusion model guided by neural operators for continuous remote sensing image super-resolution (NeurOp-Diff). Neural operators are used to learn resolution representations at arbitrary scales, encoding low-resolution (LR) images into high-dimensional features, which are then used as prior conditions to guide the diffusion model for denoising. This effectively addresses the artifacts and excessive smoothing issues present in existing super-resolution (SR) methods, enabling the generation of high-quality, continuous super-resolution images. Specifically, we adjust the super-resolution scale by a scaling factor s, allowing the model to adapt to different super-resolution magnifications. Furthermore, experiments on multiple datasets demonstrate the effectiveness of NeurOp-Diff. Our code is available at https://github.com/zerono000/NeurOp-Diff.","2025-01-17","2025-04-11 01:56:56","2025-04-11 01:57:02","2025-04-11 01:56:56","","","","","","","NeurOp-Diff","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2501.09054 [eess]","","/Users/alihaider/Zotero/storage/JPSXX4VM/Xu et al. - 2025 - NeurOp-DiffContinuous Remote Sensing Image Super-Resolution via Neural Operator Diffusion.pdf; /Users/alihaider/Zotero/storage/GWTWGZSZ/2501.html","","","Electrical Engineering and Systems Science - Image and Video Processing; Computer Science - Graphics","","","","","","","","","","","","","","","","","","","arXiv:2501.09054","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V9GG6MF9","preprint","2024","Ma, Zhiyuan; Zhang, Yuzhu; Jia, Guoli; Zhao, Liangliang; Ma, Yichao; Ma, Mingjie; Liu, Gaofeng; Zhang, Kaiyan; Li, Jianjun; Zhou, Bowen","Efficient Diffusion Models: A Comprehensive Survey from Principles to Practices","","","","10.48550/arXiv.2410.11795","http://arxiv.org/abs/2410.11795","As one of the most popular and sought-after generative models in the recent years, diffusion models have sparked the interests of many researchers and steadily shown excellent advantage in various generative tasks such as image synthesis, video generation, molecule design, 3D scene rendering and multimodal generation, relying on their dense theoretical principles and reliable application practices. The remarkable success of these recent efforts on diffusion models comes largely from progressive design principles and efficient architecture, training, inference, and deployment methodologies. However, there has not been a comprehensive and in-depth review to summarize these principles and practices to help the rapid understanding and application of diffusion models. In this survey, we provide a new efficiency-oriented perspective on these existing efforts, which mainly focuses on the profound principles and efficient practices in architecture designs, model training, fast inference and reliable deployment, to guide further theoretical research, algorithm migration and model application for new scenarios in a reader-friendly way. \url{https://github.com/ponyzym/Efficient-DMs-Survey}","2024-10-16","2025-04-11 01:59:16","2025-04-11 01:59:19","2025-04-11 01:59:16","","","","","","","Efficient Diffusion Models","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2410.11795 [cs]","","/Users/alihaider/Zotero/storage/XJPF4YGV/Ma et al. - 2024 - Efficient Diffusion Models A Comprehensive Survey from Principles to Practices.pdf; /Users/alihaider/Zotero/storage/NQUGXMV2/2410.html","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2410.11795","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2BNL9P3C","preprint","2024","Xu, Katherine; Zhang, Lingzhi; Shi, Jianbo","Good Seed Makes a Good Crop: Discovering Secret Seeds in Text-to-Image Diffusion Models","","","","10.48550/arXiv.2405.14828","http://arxiv.org/abs/2405.14828","Recent advances in text-to-image (T2I) diffusion models have facilitated creative and photorealistic image synthesis. By varying the random seeds, we can generate various images for a fixed text prompt. Technically, the seed controls the initial noise and, in multi-step diffusion inference, the noise used for reparameterization at intermediate timesteps in the reverse diffusion process. However, the specific impact of the random seed on the generated images remains relatively unexplored. In this work, we conduct a large-scale scientific study into the impact of random seeds during diffusion inference. Remarkably, we reveal that the best 'golden' seed achieved an impressive FID of 21.60, compared to the worst 'inferior' seed's FID of 31.97. Additionally, a classifier can predict the seed number used to generate an image with over 99.9% accuracy in just a few epochs, establishing that seeds are highly distinguishable based on generated images. Encouraged by these findings, we examined the influence of seeds on interpretable visual dimensions. We find that certain seeds consistently produce grayscale images, prominent sky regions, or image borders. Seeds also affect image composition, including object location, size, and depth. Moreover, by leveraging these 'golden' seeds, we demonstrate improved image generation such as high-fidelity inference and diversified sampling. Our investigation extends to inpainting tasks, where we uncover some seeds that tend to insert unwanted text artifacts. Overall, our extensive analyses highlight the importance of selecting good seeds and offer practical utility for image generation.","2024-05-23","2025-04-11 02:07:30","2025-04-11 02:07:35","2025-04-11 02:07:30","","","","","","","Good Seed Makes a Good Crop","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2405.14828 [cs] version: 1","","/Users/alihaider/Zotero/storage/EHDIUZKD/Xu et al. - 2024 - Good Seed Makes a Good Crop Discovering Secret Seeds in Text-to-Image Diffusion Models.pdf; /Users/alihaider/Zotero/storage/Y4WEXZYF/2405.html","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2405.14828","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VE3QNCVK","preprint","2025","Ma, Nanye; Tong, Shangyuan; Jia, Haolin; Hu, Hexiang; Su, Yu-Chuan; Zhang, Mingda; Yang, Xuan; Li, Yandong; Jaakkola, Tommi; Jia, Xuhui; Xie, Saining","Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps","","","","10.48550/arXiv.2501.09732","http://arxiv.org/abs/2501.09732","Generative models have made significant impacts across various domains, largely due to their ability to scale during training by increasing data, computational resources, and model size, a phenomenon characterized by the scaling laws. Recent research has begun to explore inference-time scaling behavior in Large Language Models (LLMs), revealing how performance can further improve with additional computation during inference. Unlike LLMs, diffusion models inherently possess the flexibility to adjust inference-time computation via the number of denoising steps, although the performance gains typically flatten after a few dozen. In this work, we explore the inference-time scaling behavior of diffusion models beyond increasing denoising steps and investigate how the generation performance can further improve with increased computation. Specifically, we consider a search problem aimed at identifying better noises for the diffusion sampling process. We structure the design space along two axes: the verifiers used to provide feedback, and the algorithms used to find better noise candidates. Through extensive experiments on class-conditioned and text-conditioned image generation benchmarks, our findings reveal that increasing inference-time compute leads to substantial improvements in the quality of samples generated by diffusion models, and with the complicated nature of images, combinations of the components in the framework can be specifically chosen to conform with different application scenario.","2025-01-16","2025-04-11 02:10:41","2025-04-11 02:10:43","2025-04-11 02:10:41","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2501.09732 [cs]","","/Users/alihaider/Zotero/storage/6ICMMPAJ/Ma et al. - 2025 - Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps.pdf; /Users/alihaider/Zotero/storage/SX4QPU42/Pagoda_Diffusion.pdf; /Users/alihaider/Zotero/storage/7N9STAH9/2501.html","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2501.09732","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZJANIHQH","preprint","2023","Go, Hyojun; Kim, JinYoung; Lee, Yunsung; Lee, Seunghyun; Oh, Shinhyeok; Moon, Hyeongdon; Choi, Seungtaek","Addressing Negative Transfer in Diffusion Models","","","","10.48550/arXiv.2306.00354","http://arxiv.org/abs/2306.00354","Diffusion-based generative models have achieved remarkable success in various domains. It trains a shared model on denoising tasks that encompass different noise levels simultaneously, representing a form of multi-task learning (MTL). However, analyzing and improving diffusion models from an MTL perspective remains under-explored. In particular, MTL can sometimes lead to the well-known phenomenon of negative transfer, which results in the performance degradation of certain tasks due to conflicts between tasks. In this paper, we first aim to analyze diffusion training from an MTL standpoint, presenting two key observations: (O1) the task affinity between denoising tasks diminishes as the gap between noise levels widens, and (O2) negative transfer can arise even in diffusion training. Building upon these observations, we aim to enhance diffusion training by mitigating negative transfer. To achieve this, we propose leveraging existing MTL methods, but the presence of a huge number of denoising tasks makes this computationally expensive to calculate the necessary per-task loss or gradient. To address this challenge, we propose clustering the denoising tasks into small task clusters and applying MTL methods to them. Specifically, based on (O2), we employ interval clustering to enforce temporal proximity among denoising tasks within clusters. We show that interval clustering can be solved using dynamic programming, utilizing signal-to-noise ratio, timestep, and task affinity for clustering objectives. Through this, our approach addresses the issue of negative transfer in diffusion models by allowing for efficient computation of MTL methods. We validate the efficacy of proposed clustering and its integration with MTL methods through various experiments, demonstrating 1) improved generation quality and 2) faster training convergence of diffusion models.","2023-12-30","2025-04-11 02:19:04","2025-04-11 02:19:05","2025-04-11 02:19:04","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2306.00354 [cs]","","/Users/alihaider/Zotero/storage/FWHKID8L/Go et al. - 2023 - Addressing Negative Transfer in Diffusion Models.pdf; /Users/alihaider/Zotero/storage/A9KSTI96/2306.html","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2306.00354","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"85D3LRSN","preprint","2024","Shi, Zhenning; Zheng, Haoshuai; Xu, Chen; Dong, Changsheng; Pan, Bin; Xie, Xueshuo; He, Along; Li, Tao; Fu, Huazhu","Resfusion: Denoising Diffusion Probabilistic Models for Image Restoration Based on Prior Residual Noise","","","","10.48550/arXiv.2311.14900","http://arxiv.org/abs/2311.14900","Recently, research on denoising diffusion models has expanded its application to the field of image restoration. Traditional diffusion-based image restoration methods utilize degraded images as conditional input to effectively guide the reverse generation process, without modifying the original denoising diffusion process. However, since the degraded images already include low-frequency information, starting from Gaussian white noise will result in increased sampling steps. We propose Resfusion, a general framework that incorporates the residual term into the diffusion forward process, starting the reverse process directly from the noisy degraded images. The form of our inference process is consistent with the DDPM. We introduced a weighted residual noise, named resnoise, as the prediction target and explicitly provide the quantitative relationship between the residual term and the noise term in resnoise. By leveraging a smooth equivalence transformation, Resfusion determine the optimal acceleration step and maintains the integrity of existing noise schedules, unifying the training and inference processes. The experimental results demonstrate that Resfusion exhibits competitive performance on ISTD dataset, LOL dataset and Raindrop dataset with only five sampling steps. Furthermore, Resfusion can be easily applied to image generation and emerges with strong versatility. Our code and model are available at https://github.com/nkicsl/Resfusion.","2024-10-24","2025-04-11 02:19:57","2025-04-11 02:19:57","2025-04-11 02:19:57","","","","","","","Resfusion","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2311.14900 [cs]","","/Users/alihaider/Zotero/storage/UDLP9THS/Shi et al. - 2024 - Resfusion Denoising Diffusion Probabilistic Models for Image Restoration Based on Prior Residual No.pdf; /Users/alihaider/Zotero/storage/SP3GMGGR/2311.html","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2311.14900","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ELWEVBDS","journalArticle","2024","Song, Bowen; Kwon, Soo Min; Zhang, Zecheng; Hu, Xinyu; Qu, Qing; Shen, Liyue","SOLVING INVERSE PROBLEMS WITH LATENT DIFFU- SION MODELS VIA HARD DATA CONSISTENCY","","","","","","Latent diffusion models have been demonstrated to generate high-quality images, while offering efficiency in model training compared to diffusion models operating in the pixel space. However, incorporating latent diffusion models to solve inverse problems remains a challenging problem due to the nonlinearity of the encoder and decoder. To address these issues, we propose ReSample, an algorithm that can solve general inverse problems with pre-trained latent diffusion models. Our algorithm incorporates data consistency by solving an optimization problem during the reverse sampling process, a concept that we term as hard data consistency. Upon solving this optimization problem, we propose a novel resampling scheme to map the measurement-consistent sample back onto the noisy data manifold and theoretically demonstrate its benefits. Lastly, we apply our algorithm to solve a wide range of linear and nonlinear inverse problems in both natural and medical images, demonstrating that our approach outperforms existing state-ofthe-art approaches, including those based on pixel-space diffusion models.","2024","2025-04-11 02:21:36","2025-04-11 02:21:36","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/alihaider/Zotero/storage/KFT89NWQ/Song et al. - 2024 - SOLVING INVERSE PROBLEMS WITH LATENT DIFFU- SION MODELS VIA HARD DATA CONSISTENCY.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"92HVFSXP","preprint","2023","Yu, Jiwen; Wang, Yinhuai; Zhao, Chen; Ghanem, Bernard; Zhang, Jian","FreeDoM: Training-Free Energy-Guided Conditional Diffusion Model","","","","10.48550/arXiv.2303.09833","http://arxiv.org/abs/2303.09833","Recently, conditional diffusion models have gained popularity in numerous applications due to their exceptional generation ability. However, many existing methods are training-required. They need to train a time-dependent classifier or a condition-dependent score estimator, which increases the cost of constructing conditional diffusion models and is inconvenient to transfer across different conditions. Some current works aim to overcome this limitation by proposing training-free solutions, but most can only be applied to a specific category of tasks and not to more general conditions. In this work, we propose a training-Free conditional Diffusion Model (FreeDoM) used for various conditions. Specifically, we leverage off-the-shelf pre-trained networks, such as a face detection model, to construct time-independent energy functions, which guide the generation process without requiring training. Furthermore, because the construction of the energy function is very flexible and adaptable to various conditions, our proposed FreeDoM has a broader range of applications than existing training-free methods. FreeDoM is advantageous in its simplicity, effectiveness, and low cost. Experiments demonstrate that FreeDoM is effective for various conditions and suitable for diffusion models of diverse data domains, including image and latent code domains.","2023-03-17","2025-04-11 02:21:41","2025-04-11 02:21:41","2025-04-11 02:21:41","","","","","","","FreeDoM","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2303.09833 [cs]","","/Users/alihaider/Zotero/storage/BBR6F4A5/Yu et al. - 2023 - FreeDoM Training-Free Energy-Guided Conditional Diffusion Model.pdf; /Users/alihaider/Zotero/storage/LEA7ECE7/2303.html","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2303.09833","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q2A9I2KJ","preprint","2024","Ganjdanesh, Alireza; Kang, Yan; Liu, Yuchen; Zhang, Richard; Lin, Zhe; Huang, Heng","Mixture of Efficient Diffusion Experts Through Automatic Interval and Sub-Network Selection","","","","10.48550/arXiv.2409.15557","http://arxiv.org/abs/2409.15557","Diffusion probabilistic models can generate high-quality samples. Yet, their sampling process requires numerous denoising steps, making it slow and computationally intensive. We propose to reduce the sampling cost by pruning a pretrained diffusion model into a mixture of efficient experts. First, we study the similarities between pairs of denoising timesteps, observing a natural clustering, even across different datasets. This suggests that rather than having a single model for all time steps, separate models can serve as ``experts'' for their respective time intervals. As such, we separately fine-tune the pretrained model on each interval, with elastic dimensions in depth and width, to obtain experts specialized in their corresponding denoising interval. To optimize the resource usage between experts, we introduce our Expert Routing Agent, which learns to select a set of proper network configurations. By doing so, our method can allocate the computing budget between the experts in an end-to-end manner without requiring manual heuristics. Finally, with a selected configuration, we fine-tune our pruned experts to obtain our mixture of efficient experts. We demonstrate the effectiveness of our method, DiffPruning, across several datasets, LSUN-Church, LSUN-Beds, FFHQ, and ImageNet, on the Latent Diffusion Model architecture.","2024-09-23","2025-04-11 02:22:21","2025-04-11 02:22:21","2025-04-11 02:22:21","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2409.15557 [cs]","","/Users/alihaider/Zotero/storage/JWTUJY7K/Ganjdanesh et al. - 2024 - Mixture of Efficient Diffusion Experts Through Automatic Interval and Sub-Network Selection.pdf; /Users/alihaider/Zotero/storage/LSGVCTM2/2409.html","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2409.15557","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6RABHK2Q","preprint","2023","Zhao, Wenliang; Bai, Lujia; Rao, Yongming; Zhou, Jie; Lu, Jiwen","UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models","","","","10.48550/arXiv.2302.04867","http://arxiv.org/abs/2302.04867","Diffusion probabilistic models (DPMs) have demonstrated a very promising ability in high-resolution image synthesis. However, sampling from a pre-trained DPM is time-consuming due to the multiple evaluations of the denoising network, making it more and more important to accelerate the sampling of DPMs. Despite recent progress in designing fast samplers, existing methods still cannot generate satisfying images in many applications where fewer steps (e.g., $<$10) are favored. In this paper, we develop a unified corrector (UniC) that can be applied after any existing DPM sampler to increase the order of accuracy without extra model evaluations, and derive a unified predictor (UniP) that supports arbitrary order as a byproduct. Combining UniP and UniC, we propose a unified predictor-corrector framework called UniPC for the fast sampling of DPMs, which has a unified analytical form for any order and can significantly improve the sampling quality over previous methods, especially in extremely few steps. We evaluate our methods through extensive experiments including both unconditional and conditional sampling using pixel-space and latent-space DPMs. Our UniPC can achieve 3.87 FID on CIFAR10 (unconditional) and 7.51 FID on ImageNet 256$\times$256 (conditional) with only 10 function evaluations. Code is available at https://github.com/wl-zhao/UniPC.","2023-10-17","2025-04-11 02:27:12","2025-04-11 02:27:14","2025-04-11 02:27:12","","","","","","","UniPC","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2302.04867 [cs]","","/Users/alihaider/Zotero/storage/G2Q4SM7B/Zhao et al. - 2023 - UniPC A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models.pdf; /Users/alihaider/Zotero/storage/GKTXBXKK/Jiang_SCEdit_Efficient_and_Controllable_Image_Diffusion_Generation_via_Skip_Connection_CVPR_2024_paper.pdf; /Users/alihaider/Zotero/storage/8RB3KZYT/2302.html","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2302.04867","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KHLVEDL4","journalArticle","","Cha, Junghun; Haider, Ali; Yang, Seoyun; Jin, Hoeyeong; Yang, Subin","Descanning: From Scanned to the Original Images with a Color Correction Diffusion Model","","","","","","A significant volume of analog information, i.e., documents and images, have been digitized in the form of scanned copies for storing, sharing, and/or analyzing in the digital world. However, the quality of such contents is severely degraded by various distortions caused by printing, storing, and scanning processes in the physical world. Although restoring highquality content from scanned copies has become an indispensable task for many products, it has not been systematically explored, and to the best of our knowledge, no public datasets are available. In this paper, we define this problem as Descanning and introduce a new high-quality and largescale dataset named DESCAN-18K. It contains 18K pairs of original and scanned images collected in the wild containing multiple complex degradations. In order to eliminate such complex degradations, we propose a new image restoration model called DescanDiffusion consisting of a color encoder that corrects the global color degradation and a conditional denoising diffusion probabilistic model (DDPM) that removes local degradations. To further improve the generalization ability of DescanDiffusion, we also design a synthetic data generation scheme by reproducing prominent degradations in scanned images. We demonstrate that our DescanDiffusion outperforms other baselines including commercial restoration products, objectively and subjectively, via comprehensive experiments and analyses.","","2025-04-11 02:33:28","2025-04-11 02:33:28","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/alihaider/Zotero/storage/Z8SG6GIR/Cha et al. - Descanning From Scanned to the Original Images with a Color Correction Diffusion Model.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"67Y5MPZ3","preprint","2025","Becker, Alexander; Daudt, Rodrigo Caye; Narnhofer, Dominik; Peters, Torben; Metzger, Nando; Wegner, Jan Dirk; Schindler, Konrad","Thera: Aliasing-Free Arbitrary-Scale Super-Resolution with Neural Heat Fields","","","","10.48550/arXiv.2311.17643","http://arxiv.org/abs/2311.17643","Recent approaches for arbitrary-scale single image superresolution (ASSR) have used local neural fields to represent continuous signals that can be sampled at arbitrary rates. However, the pointwise query of the neural field does not naturally match the point spread function (PSF) of a given pixel, which may cause aliasing in the superresolved image. We present a novel way to design neural fields such that points can be queried with an adaptive Gaussian PSF, so as to guarantee correct anti-aliasing at any desired output resolution. We achieve this with a novel activation function derived from Fourier theory. Querying points with a Gaussian PSF, compliant with sampling theory, does not incur any additional computational cost in our framework, unlike filtering in the image domain. With its theoretically guaranteed anti-aliasing, our method sets a new state of the art for ASSR, while being more parameter-efficient than previous methods. Notably, even a minimal version of our model still outperforms previous methods in most cases, while adding 2-4 orders of magnitude fewer parameters. Code and pretrained models are available at https://github.com/prs-eth/thera.","2025-03-09","2025-04-11 02:33:31","2025-04-11 02:33:31","2025-04-11 02:33:31","","","","","","","Thera","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2311.17643 [cs]","","/Users/alihaider/Zotero/storage/3G9W8KTR/Becker et al. - 2025 - Thera Aliasing-Free Arbitrary-Scale Super-Resolution with Neural Heat Fields.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2311.17643","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LSURLTB3","preprint","2024","Shi, Zhenning; Zheng, Haoshuai; Xu, Chen; Dong, Changsheng; Pan, Bin; Xie, Xueshuo; He, Along; Li, Tao; Fu, Huazhu","Resfusion: Denoising Diffusion Probabilistic Models for Image Restoration Based on Prior Residual Noise","","","","10.48550/arXiv.2311.14900","http://arxiv.org/abs/2311.14900","Recently, research on denoising diffusion models has expanded its application to the field of image restoration. Traditional diffusion-based image restoration methods utilize degraded images as conditional input to effectively guide the reverse generation process, without modifying the original denoising diffusion process. However, since the degraded images already include low-frequency information, starting from Gaussian white noise will result in increased sampling steps. We propose Resfusion, a general framework that incorporates the residual term into the diffusion forward process, starting the reverse process directly from the noisy degraded images. The form of our inference process is consistent with the DDPM. We introduced a weighted residual noise, named resnoise, as the prediction target and explicitly provide the quantitative relationship between the residual term and the noise term in resnoise. By leveraging a smooth equivalence transformation, Resfusion determine the optimal acceleration step and maintains the integrity of existing noise schedules, unifying the training and inference processes. The experimental results demonstrate that Resfusion exhibits competitive performance on ISTD dataset, LOL dataset and Raindrop dataset with only five sampling steps. Furthermore, Resfusion can be easily applied to image generation and emerges with strong versatility. Our code and model are available at https://github.com/nkicsl/Resfusion.","2024-10-24","2025-04-11 02:33:34","2025-04-11 02:33:34","2025-04-11 02:33:34","","","","","","","Resfusion","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2311.14900 [cs]","","/Users/alihaider/Zotero/storage/LCJLG8PK/Shi et al. - 2024 - Resfusion Denoising Diffusion Probabilistic Models for Image Restoration Based on Prior Residual No.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2311.14900","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AHCBFYP4","journalArticle","","Lin, Peng; Wang, Yafei; Li, Yuanyuan; Fan, Zihao; Fu, Xianping","Underwater Color Correction Network With Knowledge Transfer","","","","","","Underwater images suffer from severe color distortion, due to the wavelength-dependent light attenuation and scattering. Various underwater image enhancement methods have been developed to improve the quality of degraded underwater images. However, contemporary approaches often overlook the impact of different scene colors on the overall process, potentially leading to undesired outcomes, such as enhanced images exhibiting excessive redness. In this paper, we observe that the color tones of degraded underwater images exhibit variability under the inﬂuence of different underwater targets and scenes. Each degraded color channel can be utilized to guide the color correction of other channels. Given this, a light-weight underwater color correction network, dubbed UCCNet, is presented to alleviate the issue of color corruption. In UCCNet, three parallel branches are designed to excavate the residual information within each color channel, subsequently leveraging these features to improve the quality of underwater images. Moreover, facing the challenge of effectively enhancing underwater images in diverse and complex scenes, the model UCCNet-KT is established based on UCCNet. In UCCNet-KT, the technology of knowledge transfer is designed to improve the generalization ability by enriching the dataset and constructing the loss function. Extensive experiments on various underwater datasets indicate the impressive performance of the UCCNet and UCCNet-KT qualitatively and quantitatively.","","2025-04-11 02:33:41","2025-04-11 02:33:41","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/alihaider/Zotero/storage/PRQWK4PR/Lin et al. - Underwater Color Correction Network With Knowledge Transfer.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B5QISESJ","preprint","2024","Park, Byeongjun; Woo, Sangmin; Go, Hyojun; Kim, Jin-Young; Kim, Changick","Denoising Task Routing for Diffusion Models","","","","10.48550/arXiv.2310.07138","http://arxiv.org/abs/2310.07138","Diffusion models generate highly realistic images by learning a multi-step denoising process, naturally embodying the principles of multi-task learning (MTL). Despite the inherent connection between diffusion models and MTL, there remains an unexplored area in designing neural architectures that explicitly incorporate MTL into the framework of diffusion models. In this paper, we present Denoising Task Routing (DTR), a simple add-on strategy for existing diffusion model architectures to establish distinct information pathways for individual tasks within a single architecture by selectively activating subsets of channels in the model. What makes DTR particularly compelling is its seamless integration of prior knowledge of denoising tasks into the framework: (1) Task Affinity: DTR activates similar channels for tasks at adjacent timesteps and shifts activated channels as sliding windows through timesteps, capitalizing on the inherent strong affinity between tasks at adjacent timesteps. (2) Task Weights: During the early stages (higher timesteps) of the denoising process, DTR assigns a greater number of task-specific channels, leveraging the insight that diffusion models prioritize reconstructing global structure and perceptually rich contents in earlier stages, and focus on simple noise removal in later stages. Our experiments reveal that DTR not only consistently boosts diffusion models’ performance across different evaluation protocols without adding extra parameters but also accelerates training convergence. Finally, we show the complementarity between our architectural approach and existing MTL optimization techniques, providing a more complete view of MTL in the context of diffusion training. Significantly, by leveraging this complementarity, we attain matched performance of DiT-XL using the smaller DiT-L with a reduction in training iterations from 7M to 2M. Our project page is available at https://byeongjun-park.github.io/DTR/.","2024-02-21","2025-04-11 02:33:43","2025-04-11 02:33:43","2025-04-11 02:33:43","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2310.07138 [cs]","","/Users/alihaider/Zotero/storage/88IKSK8U/Park et al. - 2024 - Denoising Task Routing for Diffusion Models.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2310.07138","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ESLEVN74","preprint","2023","Go, Hyojun; Kim, JinYoung; Lee, Yunsung; Lee, Seunghyun; Oh, Shinhyeok; Moon, Hyeongdon; Choi, Seungtaek","Addressing Negative Transfer in Diffusion Models","","","","10.48550/arXiv.2306.00354","http://arxiv.org/abs/2306.00354","Diffusion-based generative models have achieved remarkable success in various domains. It trains a shared model on denoising tasks that encompass different noise levels simultaneously, representing a form of multi-task learning (MTL). However, analyzing and improving diffusion models from an MTL perspective remains underexplored. In particular, MTL can sometimes lead to the well-known phenomenon of negative transfer, which results in the performance degradation of certain tasks due to conflicts between tasks. In this paper, we first aim to analyze diffusion training from an MTL standpoint, presenting two key observations: (O1) the task affinity between denoising tasks diminishes as the gap between noise levels widens, and (O2) negative transfer can arise even in diffusion training. Building upon these observations, we aim to enhance diffusion training by mitigating negative transfer. To achieve this, we propose leveraging existing MTL methods, but the presence of a huge number of denoising tasks makes this computationally expensive to calculate the necessary per-task loss or gradient. To address this challenge, we propose clustering the denoising tasks into small task clusters and applying MTL methods to them. Specifically, based on (O2), we employ interval clustering to enforce temporal proximity among denoising tasks within clusters. We show that interval clustering can be solved using dynamic programming, utilizing signal-tonoise ratio, timestep, and task affinity for clustering objectives. Through this, our approach addresses the issue of negative transfer in diffusion models by allowing for efficient computation of MTL methods. We validate the efficacy of proposed clustering and its integration with MTL methods through various experiments, demonstrating 1) improved generation quality and 2) faster training convergence of diffusion models. Our project page is available at https://gohyojun15.github. io/ANT_diffusion/.","2023-12-30","2025-04-11 02:33:44","2025-04-11 02:33:44","2025-04-11 02:33:44","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2306.00354 [cs]","","/Users/alihaider/Zotero/storage/HC6LQKAA/Go et al. - 2023 - Addressing Negative Transfer in Diffusion Models.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2306.00354","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7TPJ4A5W","preprint","2024","Park, Byeongjun; Go, Hyojun; Kim, Jin-Young; Woo, Sangmin; Ham, Seokil; Kim, Changick","Switch Diffusion Transformer: Synergizing Denoising Tasks with Sparse Mixture-of-Experts","","","","10.48550/arXiv.2403.09176","http://arxiv.org/abs/2403.09176","Diffusion models have achieved remarkable success across a range of generative tasks. Recent efforts to enhance diffusion model architectures have reimagined them as a form of multi-task learning, where each task corresponds to a denoising task at a specific noise level. While these efforts have focused on parameter isolation and task routing, they fall short of capturing detailed inter-task relationships and risk losing semantic information, respectively. In response, we introduce Switch Diffusion Transformer (Switch-DiT), which establishes inter-task relationships between conflicting tasks without compromising semantic information. To achieve this, we employ a sparse mixture-of-experts within each transformer block to utilize semantic information and facilitate handling conflicts in tasks through parameter isolation. Also, we propose a diffusion prior loss, encouraging similar tasks to share their denoising paths while isolating conflicting ones. Through these, each transformer block contains a shared expert across all tasks, where the common and task-specific denoising paths enable the diffusion model to construct its beneficial way of synergizing denoising tasks. Extensive experiments validate the effectiveness of our approach in improving both image quality and convergence rate, and further analysis demonstrates that Switch-DiT constructs tailored denoising paths across various generation scenarios. Our project page is available at https://byeongjun-park.github.io/Switch-DiT/.","2024-07-10","2025-04-11 02:33:47","2025-04-11 02:33:47","2025-04-11 02:33:47","","","","","","","Switch Diffusion Transformer","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2403.09176 [cs]","","/Users/alihaider/Zotero/storage/KQGDYI8T/Park et al. - 2024 - Switch Diffusion Transformer Synergizing Denoising Tasks with Sparse Mixture-of-Experts.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2403.09176","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MDLCBSJ6","preprint","2024","Nam, Jisu; Lee, Gyuseong; Kim, Sunwoo; Kim, Hyeonsu; Cho, Hyoungwon; Kim, Seyeon; Kim, Seungryong","Diffusion Model for Dense Matching","","","","10.48550/arXiv.2305.19094","http://arxiv.org/abs/2305.19094","The objective for establishing dense correspondence between paired images consists of two terms: a data term and a prior term. While conventional techniques focused on defining hand-designed prior terms, which are difficult to formulate, recent approaches have focused on learning the data term with deep neural networks without explicitly modeling the prior, assuming that the model itself has the capacity to learn an optimal prior from a large-scale dataset. The performance improvement was obvious, however, they often fail to address inherent ambiguities of matching, such as textureless regions, repetitive patterns, large displacements, or noises. To address this, we propose DiffMatch, a novel conditional diffusion-based framework designed to explicitly model both the data and prior terms for dense matching. This is accomplished by leveraging a conditional denoising diffusion model that explicitly takes matching cost and injects the prior within generative process. However, limited input resolution of the diffusion model is a major hindrance. We address this with a cascaded pipeline, starting with a low-resolution model, followed by a super-resolution model that successively upsamples and incorporates finer details to the matching field. Our experimental results demonstrate significant performance improvements of our method over existing approaches, and the ablation studies validate our design choices along with the effectiveness of each component. Code and pretrained weights are available at https://ku-cvlab.github.io/DiffMatch.","2024-01-25","2025-04-11 02:33:50","2025-04-11 02:33:50","2025-04-11 02:33:50","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2305.19094 [cs]","","/Users/alihaider/Zotero/storage/54F8KT42/Nam et al. - 2024 - Diffusion Model for Dense Matching.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2305.19094","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5WHSLNA9","conferencePaper","2024","Hui, Mude; Wei, Zihao; Zhu, Hongru; Xia, Fei; Zhou, Yuyin","MicroDiffusion: Implicit Representation-Guided Diffusion for 3D Reconstruction from Limited 2D Microscopy Projections","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","979-8-3503-5300-6","","10.1109/CVPR52733.2024.01089","https://ieeexplore.ieee.org/document/10656255/","Volumetric optical microscopy using non-diffracting beams enables rapid imaging of 3D volumes by projecting them axially to 2D images but lacks crucial depth information. Addressing this, we introduce MicroDiffusion, a pioneering tool facilitating high-quality, depth-resolved 3D volume reconstruction from limited 2D projections. While existing Implicit Neural Representation (INR) models often yield incomplete outputs and Denoising Diffusion Probabilistic Models (DDPM) excel at capturing details, our method integrates INR’s structural coherence with DDPM’s fine-detail enhancement capabilities. We pretrain an INR model to transform 2D axially-projected images into a preliminary 3D volume. This pretrained INR acts as a global prior guiding DDPM’s generative process through a linear interpolation between INR outputs and noise inputs. This strategy enriches the diffusion process with structured 3D information, enhancing detail and reducing noise in localized 2D images. By conditioning the diffusion model on the closest 2D projection, MicroDiffusion substantially enhances fidelity in resulting 3D reconstructions, surpassing INR and standard DDPM outputs with unparalleled image quality and structural fidelity. Our code and dataset are available at https://github.com/UCSC- VLAA/ MicroDiffusion.","2024-06-16","2025-04-11 02:40:25","2025-04-11 02:40:25","2025-04-11 02:40:25","11460-11469","","","","","","MicroDiffusion","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/alihaider/Zotero/storage/2V7U2QSA/Hui et al. - 2024 - MicroDiffusion Implicit Representation-Guided Diffusion for 3D Reconstruction from Limited 2D Micro.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"JWUJWSGB","journalArticle","","Jiang, Zeyinzi; Mao, Chaojie; Pan, Yulin; Han, Zhen; Zhang, Jingfeng","SCEdit: Efficient and Controllable Image Diffusion Generation via Skip Connection Editing","","","","","","","","2025-04-11 02:48:19","2025-04-11 02:48:19","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/alihaider/Zotero/storage/FYSJNPCU/Jiang et al. - SCEdit Efficient and Controllable Image Diffusion Generation via Skip Connection Editing.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FQEKSTWC","preprint","2024","Kim, Dongjun; Lai, Chieh-Hsin; Liao, Wei-Hsiang; Takida, Yuhta; Murata, Naoki; Uesaka, Toshimitsu; Mitsufuji, Yuki; Ermon, Stefano","PaGoDA: Progressive Growing of a One-Step Generator from a Low-Resolution Diffusion Teacher","","","","10.48550/arXiv.2405.14822","http://arxiv.org/abs/2405.14822","The diffusion model performs remarkable in generating high-dimensional content but is computationally intensive, especially during training. We propose Progressive Growing of Diffusion Autoencoder (PaGoDA), a novel pipeline that reduces the training costs through three stages: training diffusion on downsampled data, distilling the pretrained diffusion, and progressive super-resolution. With the proposed pipeline, PaGoDA achieves a 64× reduced cost in training its diffusion model on 8× downsampled data; while at the inference, with the single-step, it performs state-of-the-art on ImageNet across all resolutions from 64 × 64 to 512 × 512, and text-to-image. PaGoDA’s pipeline can be applied directly in the latent space, adding compression alongside the pre-trained autoencoder in Latent Diffusion Models (e.g., Stable Diffusion). The code is available at https://github.com/sony/pagoda.","2024-10-29","2025-04-11 02:49:19","2025-04-11 02:49:19","2025-04-11 02:49:19","","","","","","","PaGoDA","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2405.14822 [cs]","","/Users/alihaider/Zotero/storage/6H8WSTZF/Kim et al. - 2024 - PaGoDA Progressive Growing of a One-Step Generator from a Low-Resolution Diffusion Teacher.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2405.14822","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VETMLYSN","journalArticle","","Tragakis, Athanasios; Aversa, Marco; Kaul, Chaitanya; Murray-Smith, Roderick; Faccio, Daniele","Is One GPU Enough? Pushing Image Generation at Higher-Resolutions with Foundation Models.","","","","","","","","2025-04-11 02:49:52","2025-04-11 02:49:52","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/alihaider/Zotero/storage/BNFJCCVM/Tragakis et al. - Is One GPU Enough Pushing Image Generation at Higher-Resolutions with Foundation Models..pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U38P4Y5H","preprint","2025","Chen, Dengsheng; Hu, Jie; Wei, Xiaoming; Wu, Enhua","Denoising with a Joint-Embedding Predictive Architecture","","","","10.48550/arXiv.2410.03755","http://arxiv.org/abs/2410.03755","Joint-embedding predictive architectures (JEPAs) have shown substantial promise in self-supervised representation learning, yet their application in generative modeling remains underexplored. Conversely, diffusion models have demonstrated significant efficacy in modeling arbitrary probability distributions. In this paper, we introduce Denoising with a Joint-Embedding Predictive Architecture (D-JEPA), pioneering the integration of JEPA within generative modeling. By recognizing JEPA as a form of masked image modeling, we reinterpret it as a generalized next-token prediction strategy, facilitating data generation in an auto-regressive manner. Furthermore, we incorporate diffusion loss to model the per-token probability distribution, enabling data generation in a continuous space. We also adapt flow matching loss as an alternative to diffusion loss, thereby enhancing the flexibility of D-JEPA. Empirically, with increased GFLOPs, D-JEPA consistently achieves lower FID scores with fewer training epochs, indicating its good scalability. Our base, large, and huge models outperform all previous generative models across all scales on ImageNet conditional generation benchmarks. Beyond image generation, D-JEPA is well-suited for other continuous data modeling, including video and audio.","2025-02-04","2025-04-15 02:05:27","2025-04-15 02:05:33","2025-04-15 02:05:27","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2410.03755 [cs]","","/Users/alihaider/Zotero/storage/V253QI8P/Chen et al. - 2025 - Denoising with a Joint-Embedding Predictive Architecture.pdf; /Users/alihaider/Zotero/storage/TWYIXCA3/2410.html","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2410.03755","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5GZF9WB7","preprint","2025","Abud, Khaled; Lavrushkin, Sergey; Kirillov, Alexey; Vatolin, Dmitriy","IQA-Adapter: Exploring Knowledge Transfer from Image Quality Assessment to Diffusion-based Generative Models","","","","10.48550/arXiv.2412.01794","http://arxiv.org/abs/2412.01794","Diffusion-based models have recently revolutionized image generation, achieving unprecedented levels of fidelity. However, consistent generation of high-quality images remains challenging partly due to the lack of conditioning mechanisms for perceptual quality. In this work, we propose methods to integrate image quality assessment (IQA) models into diffusion-based generators, enabling quality-aware image generation. We show that diffusion models can learn complex qualitative relationships from both IQA models' outputs and internal activations. First, we experiment with gradient-based guidance to optimize image quality directly and show this method has limited generalizability. To address this, we introduce IQA-Adapter, a novel framework that conditions generation on target quality levels by learning the implicit relationship between images and quality scores. When conditioned on high target quality, IQA-Adapter can shift the distribution of generated images towards a higher-quality subdomain, and, inversely, it can be used as a degradation model, generating progressively more distorted images when provided with a lower-quality signal. Under high-quality condition, IQA-Adapter achieves up to a 10% improvement across multiple objective metrics, as confirmed by a user preference study, while preserving generative diversity and content. Furthermore, we extend IQA-Adapter to a reference-based conditioning scenario, utilizing the rich activation space of IQA models to transfer highly specific, content-agnostic qualitative features between images.","2025-03-16","2025-04-18 01:36:37","2025-04-18 01:36:37","2025-04-18 01:36:37","","","","","","","IQA-Adapter","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2412.01794 [cs]","","/Users/alihaider/Zotero/storage/24HMZYEA/Abud et al. - 2025 - IQA-Adapter Exploring Knowledge Transfer from Image Quality Assessment to Diffusion-based Generativ.pdf; /Users/alihaider/Zotero/storage/7DA8CUCR/2412.html","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2412.01794","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D2EJY85F","preprint","2024","Yue, Zongsheng; Wang, Jianyi; Loy, Chen Change","Efficient Diffusion Model for Image Restoration by Residual Shifting","","","","10.48550/arXiv.2403.07319","http://arxiv.org/abs/2403.07319","While diffusion-based image restoration (IR) methods have achieved remarkable success, they are still limited by the low inference speed attributed to the necessity of executing hundreds or even thousands of sampling steps. Existing acceleration sampling techniques, though seeking to expedite the process, inevitably sacrifice performance to some extent, resulting in over-blurry restored outcomes. To address this issue, this study proposes a novel and efficient diffusion model for IR that significantly reduces the required number of diffusion steps. Our method avoids the need for post-acceleration during inference, thereby avoiding the associated performance deterioration. Specifically, our proposed method establishes a Markov chain that facilitates the transitions between the high-quality and low-quality images by shifting their residuals, substantially improving the transition efficiency. A carefully formulated noise schedule is devised to flexibly control the shifting speed and the noise strength during the diffusion process. Extensive experimental evaluations demonstrate that the proposed method achieves superior or comparable performance to current state-of-the-art methods on three classical IR tasks, namely image super-resolution, image inpainting, and blind face restoration, \textit{\textbf{even only with four sampling steps}}. Our code and model are publicly available at \url{https://github.com/zsyOAOA/ResShift}.","2024-11-23","2025-05-14 06:08:02","2025-05-14 06:08:02","2025-05-14 06:08:02","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2403.07319 [cs]","","/Users/alihaider/Zotero/storage/DUJTUJZ6/Yue et al. - 2024 - Efficient Diffusion Model for Image Restoration by Residual Shifting.pdf; /Users/alihaider/Zotero/storage/2QQ2TMFA/2403.html","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2403.07319","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FFCDZR83","preprint","2024","Xiong, Bing; Peng, Yue; Zhang, RanRan; Chen, Fuqiang; He, JiaYe; Qin, Wenjian","Unpaired Multi-Domain Histopathology Virtual Staining using Dual Path Prompted Inversion","","","","10.48550/arXiv.2412.11106","http://arxiv.org/abs/2412.11106","Virtual staining leverages computer-aided techniques to transfer the style of histochemically stained tissue samples to other staining types. In virtual staining of pathological images, maintaining strict structural consistency is crucial, as these images emphasize structural integrity more than natural images. Even slight structural alterations can lead to deviations in diagnostic semantic information. Furthermore, the unpaired characteristic of virtual staining data may compromise the preservation of pathological diagnostic content. To address these challenges, we propose a dual-path inversion virtual staining method using prompt learning, which optimizes visual prompts to control content and style, while preserving complete pathological diagnostic content. Our proposed inversion technique comprises two key components: (1) Dual Path Prompted Strategy, we utilize a feature adapter function to generate reference images for inversion, providing style templates for input image inversion, called Style Target Path. We utilize the inversion of the input image as the Structural Target path, employing visual prompt images to maintain structural consistency in this path while preserving style information from the style Target path. During the deterministic sampling process, we achieve complete content-style disentanglement through a plug-and-play embedding visual prompt approach. (2) StainPrompt Optimization, where we only optimize the null visual prompt as ``operator'' for dual path inversion, rather than fine-tune pre-trained model. We optimize null visual prompt for structual and style trajectory around pivotal noise on each timestep, ensuring accurate dual-path inversion reconstruction. Extensive evaluations on publicly available multi-domain unpaired staining datasets demonstrate high structural consistency and accurate style transfer results.","2024-12-15","2025-05-19 08:04:29","2025-05-19 08:04:29","2025-05-19 08:04:29","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2412.11106 [eess]","","/Users/alihaider/Zotero/storage/LT5HE67Z/Xiong et al. - 2024 - Unpaired Multi-Domain Histopathology Virtual Staining using Dual Path Prompted Inversion.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Electrical Engineering and Systems Science - Image and Video Processing","","","","","","","","","","","","","","","","","","","arXiv:2412.11106","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QY3VV9SQ","preprint","2025","Stracke, Nick; Baumann, Stefan Andreas; Bauer, Kolja; Fundel, Frank; Ommer, Björn","CleanDIFT: Diffusion Features without Noise","","","","10.48550/arXiv.2412.03439","http://arxiv.org/abs/2412.03439","Internal features from large-scale pre-trained diffusion models have recently been established as powerful semantic descriptors for a wide range of downstream tasks. Works that use these features generally need to add noise to images before passing them through the model to obtain the semantic features, as the models do not offer the most useful features when given images with little to no noise. We show that this noise has a critical impact on the usefulness of these features that cannot be remedied by ensembling with different random noises. We address this issue by introducing a lightweight, unsupervised fine-tuning method that enables diffusion backbones to provide high-quality, noise-free semantic features. We show that these features readily outperform previous diffusion features by a wide margin in a wide variety of extraction setups and downstream tasks, offering better performance than even ensemble-based methods at a fraction of the cost.","2025-04-07","2025-05-20 06:21:46","2025-05-20 06:21:48","2025-05-20 06:21:46","","","","","","","CleanDIFT","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2412.03439 [cs]","","/Users/alihaider/Zotero/storage/AZ8KBIUD/Stracke et al. - 2025 - CleanDIFT Diffusion Features without Noise.pdf; /Users/alihaider/Zotero/storage/UN7SMDNJ/2412.html","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2412.03439","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C54E5N6W","preprint","2025","Zhou, Yifan; Xiao, Zeqi; Yang, Shuai; Pan, Xingang","Alias-Free Latent Diffusion Models:Improving Fractional Shift Equivariance of Diffusion Latent Space","","","","10.48550/arXiv.2503.09419","http://arxiv.org/abs/2503.09419","Latent Diffusion Models (LDMs) are known to have an unstable generation process, where even small perturbations or shifts in the input noise can lead to significantly different outputs. This hinders their applicability in applications requiring consistent results. In this work, we redesign LDMs to enhance consistency by making them shift-equivariant. While introducing anti-aliasing operations can partially improve shift-equivariance, significant aliasing and inconsistency persist due to the unique challenges in LDMs, including 1) aliasing amplification during VAE training and multiple U-Net inferences, and 2) self-attention modules that inherently lack shift-equivariance. To address these issues, we redesign the attention modules to be shift-equivariant and propose an equivariance loss that effectively suppresses the frequency bandwidth of the features in the continuous domain. The resulting alias-free LDM (AF-LDM) achieves strong shift-equivariance and is also robust to irregular warping. Extensive experiments demonstrate that AF-LDM produces significantly more consistent results than vanilla LDM across various applications, including video editing and image-to-image translation. Code is available at: https://github.com/SingleZombie/AFLDM","2025-03-12","2025-05-20 07:04:01","2025-05-20 07:04:01","2025-05-20 07:04:01","","","","","","","Alias-Free Latent Diffusion Models","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2503.09419 [cs]","","/Users/alihaider/Zotero/storage/64IALYPV/Zhou et al. - 2025 - Alias-Free Latent Diffusion ModelsImproving Fractional Shift Equivariance of Diffusion Latent Space.pdf; /Users/alihaider/Zotero/storage/L5T97VC9/2503.html","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2503.09419","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KSY8R7LT","preprint","2024","Lan, Guanzhou; Ma, Qianli; Yang, Yuqi; Wang, Zhigang; Wang, Dong; Li, Xuelong; Zhao, Bin","Efficient Diffusion as Low Light Enhancer","","","","10.48550/arXiv.2410.12346","http://arxiv.org/abs/2410.12346","The computational burden of the iterative sampling process remains a major challenge in diffusion-based Low-Light Image Enhancement (LLIE). Current acceleration methods, whether training-based or training-free, often lead to significant performance degradation, highlighting the trade-off between performance and efficiency. In this paper, we identify two primary factors contributing to performance degradation: fitting errors and the inference gap. Our key insight is that fitting errors can be mitigated by linearly extrapolating the incorrect score functions, while the inference gap can be reduced by shifting the Gaussian flow to a reflectance-aware residual space. Based on the above insights, we design Reflectance-Aware Trajectory Refinement (RATR) module, a simple yet effective module to refine the teacher trajectory using the reflectance component of images. Following this, we introduce \textbf{Re}flectance-aware \textbf{D}iffusion with \textbf{Di}stilled \textbf{T}rajectory (\textbf{ReDDiT}), an efficient and flexible distillation framework tailored for LLIE. Our framework achieves comparable performance to previous diffusion-based methods with redundant steps in just 2 steps while establishing new state-of-the-art (SOTA) results with 8 or 4 steps. Comprehensive experimental evaluations on 10 benchmark datasets validate the effectiveness of our method, consistently outperforming existing SOTA methods.","2024-11-21","2025-05-20 07:06:03","2025-05-20 07:06:04","2025-05-20 07:06:03","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2410.12346 [cs]","","/Users/alihaider/Zotero/storage/JZELN5C2/Lan et al. - 2024 - Efficient Diffusion as Low Light Enhancer.pdf; /Users/alihaider/Zotero/storage/X6YMTZA7/2410.html","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2410.12346","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BR74YFY9","preprint","2024","Fang, Gongfan; Li, Kunjun; Ma, Xinyin; Wang, Xinchao","TinyFusion: Diffusion Transformers Learned Shallow","","","","10.48550/arXiv.2412.01199","http://arxiv.org/abs/2412.01199","Diffusion Transformers have demonstrated remarkable capabilities in image generation but often come with excessive parameterization, resulting in considerable inference overhead in real-world applications. In this work, we present TinyFusion, a depth pruning method designed to remove redundant layers from diffusion transformers via end-to-end learning. The core principle of our approach is to create a pruned model with high recoverability, allowing it to regain strong performance after fine-tuning. To accomplish this, we introduce a differentiable sampling technique to make pruning learnable, paired with a co-optimized parameter to simulate future fine-tuning. While prior works focus on minimizing loss or error after pruning, our method explicitly models and optimizes the post-fine-tuning performance of pruned models. Experimental results indicate that this learnable paradigm offers substantial benefits for layer pruning of diffusion transformers, surpassing existing importance-based and error-based methods. Additionally, TinyFusion exhibits strong generalization across diverse architectures, such as DiTs, MARs, and SiTs. Experiments with DiT-XL show that TinyFusion can craft a shallow diffusion transformer at less than 7% of the pre-training cost, achieving a 2$\times$ speedup with an FID score of 2.86, outperforming competitors with comparable efficiency. Code is available at https://github.com/VainF/TinyFusion.","2024-12-02","2025-05-20 07:07:03","2025-05-20 07:07:04","2025-05-20 07:07:03","","","","","","","TinyFusion","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2412.01199 [cs]","","/Users/alihaider/Zotero/storage/T4Y28L5M/Fang et al. - 2024 - TinyFusion Diffusion Transformers Learned Shallow.pdf; /Users/alihaider/Zotero/storage/JXSVI3C2/2412.html","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2412.01199","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4IXKPC7I","preprint","2025","Lin, Shanchuan; Yang, Xiao","Diffusion Model with Perceptual Loss","","","","10.48550/arXiv.2401.00110","http://arxiv.org/abs/2401.00110","Diffusion models without guidance generate very unrealistic samples. Guidance is used ubiquitously, and previous research has attributed its effect to low-temperature sampling that improves quality by trading off diversity. However, this perspective is incomplete. Our research shows that the choice of the loss objective is the underlying reason raw diffusion models fail to generate desirable samples. In this paper, (1) our analysis shows that the loss objective plays an important role in shaping the learned distribution and the MSE loss derived from theories holds assumptions that misalign with data in practice; (2) we explain the effectiveness of guidance methods from a new perspective of perceptual supervision; (3) we validate our hypothesis by training a diffusion model with a novel self-perceptual loss objective and obtaining much more realistic samples without the need for guidance. We hope our work paves the way for future explorations of the diffusion loss objective.","2025-03-08","2025-05-23 03:48:03","2025-05-23 03:48:03","2025-05-23 03:48:03","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2401.00110 [cs]","","/Users/alihaider/Zotero/storage/X8GFAPEK/Lin and Yang - 2025 - Diffusion Model with Perceptual Loss.pdf; /Users/alihaider/Zotero/storage/5CGQTWI9/2401.html","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2401.00110","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IZR9ZSTX","preprint","2024","Liang, Zhexin; Li, Zhaochen; Zhou, Shangchen; Li, Chongyi; Loy, Chen Change","Control Color: Multimodal Diffusion-based Interactive Image Colorization","","","","10.48550/arXiv.2402.10855","http://arxiv.org/abs/2402.10855","Despite the existence of numerous colorization methods, several limitations still exist, such as lack of user interaction, inflexibility in local colorization, unnatural color rendering, insufficient color variation, and color overflow. To solve these issues, we introduce Control Color (CtrlColor), a multi-modal colorization method that leverages the pre-trained Stable Diffusion (SD) model, offering promising capabilities in highly controllable interactive image colorization. While several diffusion-based methods have been proposed, supporting colorization in multiple modalities remains non-trivial. In this study, we aim to tackle both unconditional and conditional image colorization (text prompts, strokes, exemplars) and address color overflow and incorrect color within a unified framework. Specifically, we present an effective way to encode user strokes to enable precise local color manipulation and employ a practical way to constrain the color distribution similar to exemplars. Apart from accepting text prompts as conditions, these designs add versatility to our approach. We also introduce a novel module based on self-attention and a content-guided deformable autoencoder to address the long-standing issues of color overflow and inaccurate coloring. Extensive comparisons show that our model outperforms state-of-the-art image colorization methods both qualitatively and quantitatively.","2024-02-16","2025-05-23 03:49:53","2025-05-23 03:49:53","2025-05-23 03:49:53","","","","","","","Control Color","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2402.10855 [cs]","","/Users/alihaider/Zotero/storage/SGDK8IPZ/Liang et al. - 2024 - Control Color Multimodal Diffusion-based Interactive Image Colorization.pdf; /Users/alihaider/Zotero/storage/CGGP35WC/2402.html","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2402.10855","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WIIBUMNU","preprint","2025","Shum, Ka Chun; Hua, Binh-Son; Nguyen, Duc Thanh; Yeung, Sai-Kit","Color Alignment in Diffusion","","","","10.48550/arXiv.2503.06746","http://arxiv.org/abs/2503.06746","Diffusion models have shown great promise in synthesizing visually appealing images. However, it remains challenging to condition the synthesis at a fine-grained level, for instance, synthesizing image pixels following some generic color pattern. Existing image synthesis methods often produce contents that fall outside the desired pixel conditions. To address this, we introduce a novel color alignment algorithm that confines the generative process in diffusion models within a given color pattern. Specifically, we project diffusion terms, either imagery samples or latent representations, into a conditional color space to align with the input color distribution. This strategy simplifies the prediction in diffusion models within a color manifold while still allowing plausible structures in generated contents, thus enabling the generation of diverse contents that comply with the target color pattern. Experimental results demonstrate our state-of-the-art performance in conditioning and controlling of color pixels, while maintaining on-par generation quality and diversity in comparison with regular diffusion models.","2025-03-09","2025-05-23 03:52:20","2025-05-23 03:52:20","2025-05-23 03:52:20","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2503.06746 [cs]","","/Users/alihaider/Zotero/storage/4NLG3LWN/Shum et al. - 2025 - Color Alignment in Diffusion.pdf; /Users/alihaider/Zotero/storage/ANXJNUZ2/2503.html","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2503.06746","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UGQS28SR","preprint","2023","Li, Xiuyu; Liu, Yijiang; Lian, Long; Yang, Huanrui; Dong, Zhen; Kang, Daniel; Zhang, Shanghang; Keutzer, Kurt","Q-Diffusion: Quantizing Diffusion Models","","","","10.48550/arXiv.2302.04304","http://arxiv.org/abs/2302.04304","Diffusion models have achieved great success in image synthesis through iterative noise estimation using deep neural networks. However, the slow inference, high memory consumption, and computation intensity of the noise estimation model hinder the efficient adoption of diffusion models. Although post-training quantization (PTQ) is considered a go-to compression method for other tasks, it does not work out-of-the-box on diffusion models. We propose a novel PTQ method specifically tailored towards the unique multitimestep pipeline and model architecture of the diffusion models, which compresses the noise estimation network to accelerate the generation process. We identify the key difficulty of diffusion model quantization as the changing output distributions of noise estimation networks over multiple time steps and the bimodal activation distribution of the shortcut layers within the noise estimation network. We tackle these challenges with timestep-aware calibration and split shortcut quantization in this work. Experimental results show that our proposed method is able to quantize full-precision unconditional diffusion models into 4-bit while maintaining comparable performance (small FID change of at most 2.34 compared to >100 for traditional PTQ) in a training-free manner. Our approach can also be applied to text-guided image generation, where we can run stable diffusion in 4-bit weights with high generation quality for the first time.","2023-06-08","2025-05-26 10:08:45","2025-05-26 10:08:45","2025-05-26 10:08:45","","","","","","","Q-Diffusion","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2302.04304 [cs]","","/Users/alihaider/Zotero/storage/2ENH6IH2/Li et al. - 2023 - Q-Diffusion Quantizing Diffusion Models.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2302.04304","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TWA29WGZ","preprint","2024","Cai, Xin; You, Zhiyuan; Zhang, Hailong; Liu, Wentao; Gu, Jinwei; Xue, Tianfan","PhoCoLens: Photorealistic and Consistent Reconstruction in Lensless Imaging","","","","10.48550/arXiv.2409.17996","http://arxiv.org/abs/2409.17996","Lensless cameras offer significant advantages in size, weight, and cost compared to traditional lens-based systems. Without a focusing lens, lensless cameras rely on computational algorithms to recover the scenes from multiplexed measurements. However, current algorithms struggle with inaccurate forward imaging models and insufficient priors to reconstruct high-quality images. To overcome these limitations, we introduce a novel two-stage approach for consistent and photorealistic lensless image reconstruction. The first stage of our approach ensures data consistency by focusing on accurately reconstructing the low-frequency content with a spatially varying deconvolution method that adjusts to changes in the Point Spread Function (PSF) across the camera's field of view. The second stage enhances photorealism by incorporating a generative prior from pre-trained diffusion models. By conditioning on the low-frequency content retrieved in the first stage, the diffusion model effectively reconstructs the high-frequency details that are typically lost in the lensless imaging process, while also maintaining image fidelity. Our method achieves a superior balance between data fidelity and visual quality compared to existing methods, as demonstrated with two popular lensless systems, PhlatCam and DiffuserCam. Project website: https://phocolens.github.io/.","2024-10-07","2025-05-26 10:30:45","2025-05-26 10:30:45","2025-05-26 10:30:45","","","","","","","PhoCoLens","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2409.17996 [eess]","","/Users/alihaider/Zotero/storage/EWGDMELF/Cai et al. - 2024 - PhoCoLens Photorealistic and Consistent Reconstruction in Lensless Imaging.pdf; /Users/alihaider/Zotero/storage/NANWAVVJ/2409.html","","","Computer Science - Computer Vision and Pattern Recognition; Electrical Engineering and Systems Science - Image and Video Processing; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2409.17996","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4UPQIKDI","preprint","2025","Lee, MinKyu; Hyun, Sangeek; Jun, Woojin; Heo, Jae-Pil","Auto-Encoded Supervision for Perceptual Image Super-Resolution","","","","10.48550/arXiv.2412.00124","http://arxiv.org/abs/2412.00124","This work tackles the fidelity objective in the perceptual super-resolution~(SR). Specifically, we address the shortcomings of pixel-level $L_\text{p}$ loss ($\mathcal{L}_\text{pix}$) in the GAN-based SR framework. Since $L_\text{pix}$ is known to have a trade-off relationship against perceptual quality, prior methods often multiply a small scale factor or utilize low-pass filters. However, this work shows that these circumventions fail to address the fundamental factor that induces blurring. Accordingly, we focus on two points: 1) precisely discriminating the subcomponent of $L_\text{pix}$ that contributes to blurring, and 2) only guiding based on the factor that is free from this trade-off relationship. We show that they can be achieved in a surprisingly simple manner, with an Auto-Encoder (AE) pretrained with $L_\text{pix}$. Accordingly, we propose the Auto-Encoded Supervision for Optimal Penalization loss ($L_\text{AESOP}$), a novel loss function that measures distance in the AE space, instead of the raw pixel space. Note that the AE space indicates the space after the decoder, not the bottleneck. By simply substituting $L_\text{pix}$ with $L_\text{AESOP}$, we can provide effective reconstruction guidance without compromising perceptual quality. Designed for simplicity, our method enables easy integration into existing SR frameworks. Experimental results verify that AESOP can lead to favorable results in the perceptual SR task.","2025-04-11","2025-05-29 02:09:32","2025-05-29 02:09:32","2025-05-29 02:09:32","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2412.00124 [cs]","","/Users/alihaider/Zotero/storage/9GYDQRAP/Lee et al. - 2025 - Auto-Encoded Supervision for Perceptual Image Super-Resolution.pdf; /Users/alihaider/Zotero/storage/8SMBB8XM/2412.html","","","Computer Science - Computer Vision and Pattern Recognition; Electrical Engineering and Systems Science - Image and Video Processing","","","","","","","","","","","","","","","","","","","arXiv:2412.00124","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PH2HRFHL","preprint","2024","Fang, Gongfan; Li, Kunjun; Ma, Xinyin; Wang, Xinchao","TinyFusion: Diffusion Transformers Learned Shallow","","","","10.48550/arXiv.2412.01199","http://arxiv.org/abs/2412.01199","Diffusion Transformers have demonstrated remarkable capabilities in image generation but often come with excessive parameterization, resulting in considerable inference overhead in real-world applications. In this work, we present TinyFusion, a depth pruning method designed to remove redundant layers from diffusion transformers via end-to-end learning. The core principle of our approach is to create a pruned model with high recoverability, allowing it to regain strong performance after fine-tuning. To accomplish this, we introduce a differentiable sampling technique to make pruning learnable, paired with a co-optimized parameter to simulate future fine-tuning. While prior works focus on minimizing loss or error after pruning, our method explicitly models and optimizes the post-fine-tuning performance of pruned models. Experimental results indicate that this learnable paradigm offers substantial benefits for layer pruning of diffusion transformers, surpassing existing importance-based and error-based methods. Additionally, TinyFusion exhibits strong generalization across diverse architectures, such as DiTs, MARs, and SiTs. Experiments with DiT-XL show that TinyFusion can craft a shallow diffusion transformer at less than 7% of the pre-training cost, achieving a 2$\times$ speedup with an FID score of 2.86, outperforming competitors with comparable efficiency. Code is available at https://github.com/VainF/TinyFusion.","2024-12-02","2025-05-29 02:15:18","2025-05-29 02:15:20","2025-05-29 02:15:18","","","","","","","TinyFusion","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2412.01199 [cs]","","/Users/alihaider/Zotero/storage/9NSALAZM/Fang et al. - 2024 - TinyFusion Diffusion Transformers Learned Shallow.pdf; /Users/alihaider/Zotero/storage/ZR9Q8D93/2412.html","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2412.01199","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PHWPC6HC","preprint","2025","Sun, Lingchen; Wu, Rongyuan; Ma, Zhiyuan; Liu, Shuaizheng; Yi, Qiaosi; Zhang, Lei","Pixel-level and Semantic-level Adjustable Super-resolution: A Dual-LoRA Approach","","","","10.48550/arXiv.2412.03017","http://arxiv.org/abs/2412.03017","Diffusion prior-based methods have shown impressive results in real-world image super-resolution (SR). However, most existing methods entangle pixel-level and semantic-level SR objectives in the training process, struggling to balance pixel-wise fidelity and perceptual quality. Meanwhile, users have varying preferences on SR results, thus it is demanded to develop an adjustable SR model that can be tailored to different fidelity-perception preferences during inference without re-training. We present Pixel-level and Semantic-level Adjustable SR (PiSA-SR), which learns two LoRA modules upon the pre-trained stable-diffusion (SD) model to achieve improved and adjustable SR results. We first formulate the SD-based SR problem as learning the residual between the low-quality input and the high-quality output, then show that the learning objective can be decoupled into two distinct LoRA weight spaces: one is characterized by the $\ell_2$-loss for pixel-level regression, and another is characterized by the LPIPS and classifier score distillation losses to extract semantic information from pre-trained classification and SD models. In its default setting, PiSA-SR can be performed in a single diffusion step, achieving leading real-world SR results in both quality and efficiency. By introducing two adjustable guidance scales on the two LoRA modules to control the strengths of pixel-wise fidelity and semantic-level details during inference, PiSASR can offer flexible SR results according to user preference without re-training. Codes and models can be found at https://github.com/csslc/PiSA-SR.","2025-04-03","2025-06-04 03:11:33","2025-06-04 03:11:33","2025-06-04 03:11:33","","","","","","","Pixel-level and Semantic-level Adjustable Super-resolution","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2412.03017 [cs]","","/Users/alihaider/Zotero/storage/WY6BHPJN/Sun et al. - 2025 - Pixel-level and Semantic-level Adjustable Super-resolution A Dual-LoRA Approach.pdf; /Users/alihaider/Zotero/storage/2KLUICTK/2412.html","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2412.03017","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GSHTU6FE","preprint","2025","Heinimann, Oliver; Shocher, Assaf; Zimbalist, Tal; Irani, Michal","KernelFusion: Assumption-Free Blind Super-Resolution via Patch Diffusion","","","","10.48550/arXiv.2503.21907","http://arxiv.org/abs/2503.21907","Traditional super-resolution (SR) methods assume an ``ideal'' downscaling SR-kernel (e.g., bicubic downscaling) between the high-resolution (HR) image and the low-resolution (LR) image. Such methods fail once the LR images are generated differently. Current blind-SR methods aim to remove this assumption, but are still fundamentally restricted to rather simplistic downscaling SR-kernels (e.g., anisotropic Gaussian kernels), and fail on more complex (out of distribution) downscaling degradations. However, using the correct SR-kernel is often more important than using a sophisticated SR algorithm. In ``KernelFusion'' we introduce a zero-shot diffusion-based method that makes no assumptions about the kernel. Our method recovers the unique image-specific SR-kernel directly from the LR input image, while simultaneously recovering its corresponding HR image. KernelFusion exploits the principle that the correct SR-kernel is the one that maximizes patch similarity across different scales of the LR image. We first train an image-specific patch-based diffusion model on the single LR input image, capturing its unique internal patch statistics. We then reconstruct a larger HR image with the same learned patch distribution, while simultaneously recovering the correct downscaling SR-kernel that maintains this cross-scale relation between the HR and LR images. Empirical results show that KernelFusion vastly outperforms all SR baselines on complex downscaling degradations, where existing SotA Blind-SR methods fail miserably. By breaking free from predefined kernel assumptions, KernelFusion pushes Blind-SR into a new assumption-free paradigm, handling downscaling kernels previously thought impossible.","2025-03-27","2025-06-04 05:39:25","2025-06-04 05:39:25","2025-06-04 05:39:25","","","","","","","KernelFusion","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2503.21907 [cs]","","/Users/alihaider/Zotero/storage/7A4FG88C/Heinimann et al. - 2025 - KernelFusion Assumption-Free Blind Super-Resolution via Patch Diffusion.pdf; /Users/alihaider/Zotero/storage/GB6RZ2TQ/2503.html","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2503.21907","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XHF7UTU6","preprint","2025","Shing, Makoto; Akiba, Takuya","DiffusionBlocks: Blockwise Training for Generative Models via Score-Based Diffusion","","","","10.48550/arXiv.2506.14202","http://arxiv.org/abs/2506.14202","Training large neural networks with end-to-end backpropagation creates significant memory bottlenecks, limiting accessibility to state-of-the-art AI research. We propose $\textit{DiffusionBlocks}$, a novel training framework that interprets neural network blocks as performing denoising operations in a continuous-time diffusion process. By partitioning the network into independently trainable blocks and optimizing noise level assignments based on equal cumulative probability mass, our approach achieves significant memory efficiency while maintaining competitive performance compared to traditional backpropagation in generative tasks. Experiments on image generation and language modeling tasks demonstrate memory reduction proportional to the number of blocks while achieving superior performance. DiffusionBlocks provides a promising pathway for democratizing access to large-scale neural network training with limited computational resources.","2025-06-17","2025-06-25 03:45:55","2025-06-25 03:45:57","2025-06-25 03:45:55","","","","","","","DiffusionBlocks","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2506.14202 [cs]","","/Users/alihaider/Zotero/storage/7H2BX5RF/Shing and Akiba - 2025 - DiffusionBlocks Blockwise Training for Generative Models via Score-Based Diffusion.pdf; /Users/alihaider/Zotero/storage/VFKXCAVM/2506.html","","","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2506.14202","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EXUBTUTW","preprint","2023","Go, Hyojun; Kim, JinYoung; Lee, Yunsung; Lee, Seunghyun; Oh, Shinhyeok; Moon, Hyeongdon; Choi, Seungtaek","Addressing Negative Transfer in Diffusion Models","","","","10.48550/arXiv.2306.00354","http://arxiv.org/abs/2306.00354","Diffusion-based generative models have achieved remarkable success in various domains. It trains a shared model on denoising tasks that encompass different noise levels simultaneously, representing a form of multi-task learning (MTL). However, analyzing and improving diffusion models from an MTL perspective remains under-explored. In particular, MTL can sometimes lead to the well-known phenomenon of negative transfer, which results in the performance degradation of certain tasks due to conflicts between tasks. In this paper, we first aim to analyze diffusion training from an MTL standpoint, presenting two key observations: (O1) the task affinity between denoising tasks diminishes as the gap between noise levels widens, and (O2) negative transfer can arise even in diffusion training. Building upon these observations, we aim to enhance diffusion training by mitigating negative transfer. To achieve this, we propose leveraging existing MTL methods, but the presence of a huge number of denoising tasks makes this computationally expensive to calculate the necessary per-task loss or gradient. To address this challenge, we propose clustering the denoising tasks into small task clusters and applying MTL methods to them. Specifically, based on (O2), we employ interval clustering to enforce temporal proximity among denoising tasks within clusters. We show that interval clustering can be solved using dynamic programming, utilizing signal-to-noise ratio, timestep, and task affinity for clustering objectives. Through this, our approach addresses the issue of negative transfer in diffusion models by allowing for efficient computation of MTL methods. We validate the efficacy of proposed clustering and its integration with MTL methods through various experiments, demonstrating 1) improved generation quality and 2) faster training convergence of diffusion models.","2023-12-30","2025-06-27 09:34:07","2025-06-27 09:34:10","2025-06-27 09:34:07","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2306.00354 [cs]","","/Users/alihaider/Zotero/storage/DGIHT5QN/Go et al. - 2023 - Addressing Negative Transfer in Diffusion Models.pdf; /Users/alihaider/Zotero/storage/QWUSWV85/2306.html","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2306.00354","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"32C34BPG","preprint","2024","Park, Byeongjun; Go, Hyojun; Kim, Jin-Young; Woo, Sangmin; Ham, Seokil; Kim, Changick","Switch Diffusion Transformer: Synergizing Denoising Tasks with Sparse Mixture-of-Experts","","","","10.48550/arXiv.2403.09176","http://arxiv.org/abs/2403.09176","Diffusion models have achieved remarkable success across a range of generative tasks. Recent efforts to enhance diffusion model architectures have reimagined them as a form of multi-task learning, where each task corresponds to a denoising task at a specific noise level. While these efforts have focused on parameter isolation and task routing, they fall short of capturing detailed inter-task relationships and risk losing semantic information, respectively. In response, we introduce Switch Diffusion Transformer (Switch-DiT), which establishes inter-task relationships between conflicting tasks without compromising semantic information. To achieve this, we employ a sparse mixture-of-experts within each transformer block to utilize semantic information and facilitate handling conflicts in tasks through parameter isolation. Additionally, we propose a diffusion prior loss, encouraging similar tasks to share their denoising paths while isolating conflicting ones. Through these, each transformer block contains a shared expert across all tasks, where the common and task-specific denoising paths enable the diffusion model to construct its beneficial way of synergizing denoising tasks. Extensive experiments validate the effectiveness of our approach in improving both image quality and convergence rate, and further analysis demonstrates that Switch-DiT constructs tailored denoising paths across various generation scenarios.","2024-07-10","2025-06-27 09:34:51","2025-06-27 09:34:51","2025-06-27 09:34:51","","","","","","","Switch Diffusion Transformer","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2403.09176 [cs]","","/Users/alihaider/Zotero/storage/N63QVPVV/Park et al. - 2024 - Switch Diffusion Transformer Synergizing Denoising Tasks with Sparse Mixture-of-Experts.pdf; /Users/alihaider/Zotero/storage/LJ8XMKN8/2403.html","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2403.09176","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5BMNX5B9","preprint","2025","Jain, Vineet; Sareen, Kusha; Pedramfar, Mohammad; Ravanbakhsh, Siamak","Diffusion Tree Sampling: Scalable inference-time alignment of diffusion models","","","","10.48550/arXiv.2506.20701","http://arxiv.org/abs/2506.20701","Adapting a pretrained diffusion model to new objectives at inference time remains an open problem in generative modeling. Existing steering methods suffer from inaccurate value estimation, especially at high noise levels, which biases guidance. Moreover, information from past runs is not reused to improve sample quality, resulting in inefficient use of compute. Inspired by the success of Monte Carlo Tree Search, we address these limitations by casting inference-time alignment as a search problem that reuses past computations. We introduce a tree-based approach that samples from the reward-aligned target density by propagating terminal rewards back through the diffusion chain and iteratively refining value estimates with each additional generation. Our proposed method, Diffusion Tree Sampling (DTS), produces asymptotically exact samples from the target distribution in the limit of infinite rollouts, and its greedy variant, Diffusion Tree Search (DTS$^\star$), performs a global search for high reward samples. On MNIST and CIFAR-10 class-conditional generation, DTS matches the FID of the best-performing baseline with up to $10\times$ less compute. In text-to-image generation and language completion tasks, DTS$^\star$ effectively searches for high reward samples that match best-of-N with up to $5\times$ less compute. By reusing information from previous generations, we get an anytime algorithm that turns additional compute into steadily better samples, providing a scalable approach for inference-time alignment of diffusion models.","2025-06-25","2025-06-30 05:24:09","2025-06-30 05:24:12","2025-06-30 05:24:09","","","","","","","Diffusion Tree Sampling","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2506.20701 [cs]","","/Users/alihaider/Zotero/storage/EZHEF79B/Jain et al. - 2025 - Diffusion Tree Sampling Scalable inference-time alignment of diffusion models.pdf; /Users/alihaider/Zotero/storage/EHEYAZW2/2506.html","","","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2506.20701","","","","","","","","","","","","","","","","","","","","","","","","","","",""