"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"V23MJ3VM","preprint","2025","Bae, Sangmin; Fisch, Adam; Harutyunyan, Hrayr; Ji, Ziwei; Kim, Seungyeon; Schuster, Tal","Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA","","","","10.48550/arXiv.2410.20672","http://arxiv.org/abs/2410.20672","Large language models (LLMs) are expensive to deploy. Parameter sharing offers a possible path towards reducing their size and cost, but its effectiveness in modern LLMs remains fairly limited. In this work, we revisit ""layer tying"" as form of parameter sharing in Transformers, and introduce novel methods for converting existing LLMs into smaller ""Recursive Transformers"" that share parameters across layers, with minimal loss of performance. Here, our Recursive Transformers are efficiently initialized from standard pretrained Transformers, but only use a single block of unique layers that is then repeated multiple times in a loop. We further improve performance by introducing Relaxed Recursive Transformers that add flexibility to the layer tying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still preserve the compactness of the overall model. We show that our recursive models (e.g., recursive Gemma 1B) outperform both similar-sized vanilla pretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge distillation baselines -- and can even recover most of the performance of the original ""full-size"" model (e.g., Gemma 2B with no shared parameters). Finally, we propose Continuous Depth-wise Batching, a promising new inference paradigm enabled by the Recursive Transformer when paired with early exiting. In a theoretical analysis, we show that this has the potential to lead to significant (2-3x) gains in inference throughput.","2025-02-28","2025-04-11 02:01:54","2025-04-11 02:01:54","2025-04-11 02:01:54","","","","","","","Relaxed Recursive Transformers","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2410.20672 [cs]","","/Users/alihaider/Zotero/storage/WKKFASIW/Bae et al. - 2025 - Relaxed Recursive Transformers Effective Parameter Sharing with Layer-wise LoRA.pdf; /Users/alihaider/Zotero/storage/HJ5EYAD6/2410.html","","","Computer Science - Computation and Language; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2410.20672","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CTB8EA3I","preprint","2017","Larsson, Gustav; Maire, Michael; Shakhnarovich, Gregory","FractalNet: Ultra-Deep Neural Networks without Residuals","","","","10.48550/arXiv.1605.07648","http://arxiv.org/abs/1605.07648","We introduce a design strategy for neural network macro-architecture based on self-similarity. Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals. These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers. In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks. Rather, the key may be the ability to transition, during training, from effectively shallow to deep. We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures. Such regularization allows extraction of high-performance fixed-depth subnetworks. Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.","2017-05-26","2025-04-11 02:06:21","2025-04-11 02:06:27","2025-04-11 02:06:21","","","","","","","FractalNet","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1605.07648 [cs]","","/Users/alihaider/Zotero/storage/TE6599UY/Larsson et al. - 2017 - FractalNet Ultra-Deep Neural Networks without Residuals.pdf; /Users/alihaider/Zotero/storage/SY7ZYZ6P/1605.html","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:1605.07648","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VUNZA3MT","preprint","2025","Garrido, Quentin; Ballas, Nicolas; Assran, Mahmoud; Bardes, Adrien; Najman, Laurent; Rabbat, Michael; Dupoux, Emmanuel; LeCun, Yann","Intuitive physics understanding emerges from self-supervised pretraining on natural videos","","","","10.48550/arXiv.2502.11831","http://arxiv.org/abs/2502.11831","We investigate the emergence of intuitive physics understanding in general-purpose deep neural network models trained to predict masked regions in natural videos. Leveraging the violation-of-expectation framework, we find that video prediction models trained to predict outcomes in a learned representation space demonstrate an understanding of various intuitive physics properties, such as object permanence and shape consistency. In contrast, video prediction in pixel space and multimodal large language models, which reason through text, achieve performance closer to chance. Our comparisons of these architectures reveal that jointly learning an abstract representation space while predicting missing parts of sensory input, akin to predictive coding, is sufficient to acquire an understanding of intuitive physics, and that even models trained on one week of unique video achieve above chance performance. This challenges the idea that core knowledge -- a set of innate systems to help understand the world -- needs to be hardwired to develop an understanding of intuitive physics.","2025-02-17","2025-04-15 01:21:53","2025-04-15 01:21:56","2025-04-15 01:21:53","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2502.11831 [cs]","","/Users/alihaider/Zotero/storage/R24WR2U5/Garrido et al. - 2025 - Intuitive physics understanding emerges from self-supervised pretraining on natural videos.pdf; /Users/alihaider/Zotero/storage/25F4Y57A/2502.html","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2502.11831","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BZEBYMYV","preprint","2025","Bolya, Daniel; Huang, Po-Yao; Sun, Peize; Cho, Jang Hyun; Madotto, Andrea; Wei, Chen; Ma, Tengyu; Zhi, Jiale; Rajasegaran, Jathushan; Rasheed, Hanoona; Wang, Junke; Monteiro, Marco; Xu, Hu; Dong, Shiyu; Ravi, Nikhila; Li, Daniel; Dollár, Piotr; Feichtenhofer, Christoph","Perception Encoder: The best visual embeddings are not at the output of the network","","","","10.48550/arXiv.2504.13181","http://arxiv.org/abs/2504.13181","We introduce Perception Encoder (PE), a state-of-the-art encoder for image and video understanding trained via simple vision-language learning. Traditionally, vision encoders have relied on a variety of pretraining objectives, each tailored to specific downstream tasks such as classification, captioning, or localization. Surprisingly, after scaling our carefully tuned image pretraining recipe and refining with our robust video data engine, we find that contrastive vision-language training alone can produce strong, general embeddings for all of these downstream tasks. There is only one caveat: these embeddings are hidden within the intermediate layers of the network. To draw them out, we introduce two alignment methods, language alignment for multimodal language modeling, and spatial alignment for dense prediction. Together with the core contrastive checkpoint, our PE family of models achieves state-of-the-art performance on a wide variety of tasks, including zero-shot image and video classification and retrieval; document, image, and video Q&A; and spatial tasks such as detection, depth estimation, and tracking. To foster further research, we are releasing our models, code, and a novel dataset of synthetically and human-annotated videos.","2025-04-17","2025-04-21 02:15:10","2025-04-21 02:15:17","2025-04-21 02:15:10","","","","","","","Perception Encoder","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2504.13181 [cs]","","/Users/alihaider/Zotero/storage/8DEB65RR/Bolya et al. - 2025 - Perception Encoder The best visual embeddings are not at the output of the network.pdf; /Users/alihaider/Zotero/storage/M43B5BGB/2504.html","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2504.13181","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IVCZWTGC","preprint","2023","Beyer, Lucas; Izmailov, Pavel; Kolesnikov, Alexander; Caron, Mathilde; Kornblith, Simon; Zhai, Xiaohua; Minderer, Matthias; Tschannen, Michael; Alabdulmohsin, Ibrahim; Pavetic, Filip","FlexiViT: One Model for All Patch Sizes","","","","10.48550/arXiv.2212.08013","http://arxiv.org/abs/2212.08013","Vision Transformers convert images to sequences by slicing them into patches. The size of these patches controls a speed/accuracy tradeoff, with smaller patches leading to higher accuracy at greater computational cost, but changing the patch size typically requires retraining the model. In this paper, we demonstrate that simply randomizing the patch size at training time leads to a single set of weights that performs well across a wide range of patch sizes, making it possible to tailor the model to different compute budgets at deployment time. We extensively evaluate the resulting model, which we call FlexiViT, on a wide range of tasks, including classification, image-text retrieval, open-world detection, panoptic segmentation, and semantic segmentation, concluding that it usually matches, and sometimes outperforms, standard ViT models trained at a single patch size in an otherwise identical setup. Hence, FlexiViT training is a simple drop-in improvement for ViT that makes it easy to add compute-adaptive capabilities to most models relying on a ViT backbone architecture. Code and pre-trained models are available at https://github.com/google-research/big_vision","2023-03-23","2025-05-01 02:10:16","2025-05-01 02:10:19","2025-05-01 02:10:16","","","","","","","FlexiViT","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2212.08013 [cs]","","/Users/alihaider/Zotero/storage/3LVGY8TK/Beyer et al. - 2023 - FlexiViT One Model for All Patch Sizes.pdf; /Users/alihaider/Zotero/storage/BTWPABHG/2212.html","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2212.08013","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9U7A826A","preprint","2024","Xiong, Yuwen; Li, Zhiqi; Chen, Yuntao; Wang, Feng; Zhu, Xizhou; Luo, Jiapeng; Wang, Wenhai; Lu, Tong; Li, Hongsheng; Qiao, Yu; Lu, Lewei; Zhou, Jie; Dai, Jifeng","Efficient Deformable ConvNets: Rethinking Dynamic and Sparse Operator for Vision Applications","","","","10.48550/arXiv.2401.06197","http://arxiv.org/abs/2401.06197","We introduce Deformable Convolution v4 (DCNv4), a highly efficient and effective operator designed for a broad spectrum of vision applications. DCNv4 addresses the limitations of its predecessor, DCNv3, with two key enhancements: 1. removing softmax normalization in spatial aggregation to enhance its dynamic property and expressive power and 2. optimizing memory access to minimize redundant operations for speedup. These improvements result in a significantly faster convergence compared to DCNv3 and a substantial increase in processing speed, with DCNv4 achieving more than three times the forward speed. DCNv4 demonstrates exceptional performance across various tasks, including image classification, instance and semantic segmentation, and notably, image generation. When integrated into generative models like U-Net in the latent diffusion model, DCNv4 outperforms its baseline, underscoring its possibility to enhance generative models. In practical applications, replacing DCNv3 with DCNv4 in the InternImage model to create FlashInternImage results in up to 80% speed increase and further performance improvement without further modifications. The advancements in speed and efficiency of DCNv4, combined with its robust performance across diverse vision tasks, show its potential as a foundational building block for future vision models.","2024-01-11","2025-05-01 08:51:59","2025-05-01 08:52:00","2025-05-01 08:51:59","","","","","","","Efficient Deformable ConvNets","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2401.06197 [cs]","","/Users/alihaider/Zotero/storage/HZBSL5LS/Xiong et al. - 2024 - Efficient Deformable ConvNets Rethinking Dynamic and Sparse Operator for Vision Applications.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2401.06197","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IDISSBMH","preprint","2024","Souza, Matheus; Zheng, Yidan; Kang, Kaizhang; Mishra, Yogeshwar Nath; Fu, Qiang; Heidrich, Wolfgang","Latent Space Imaging","","","","10.48550/arXiv.2407.07052","http://arxiv.org/abs/2407.07052","Digital imaging systems have classically been based on brute-force measuring and processing of pixels organized on regular grids. The human visual system, on the other hand, performs a massive data reduction from the number of photo-receptors to the optic nerve, essentially encoding the image information into a low bandwidth latent space representation suitable for processing by the human brain. In this work, we propose to follow a similar approach for the development of artificial vision systems. Latent Space Imaging is a new paradigm that, through a combination of optics and software, directly encodes the image information into the semantically rich latent space of a generative model, thus substantially reducing bandwidth and memory requirements during the capture process. We demonstrate this new principle through an initial hardware prototype based on the single pixel camera. By designing an amplitude modulation scheme that encodes into the latent space of a generative model, we achieve compression ratios from 1:100 to 1:1,000 during the imaging process, illustrating the potential of latent space imaging for highly efficient imaging hardware, to enable future applications in high speed imaging, or task-specific cameras with substantially reduced hardware complexity.","2024-07-09","2025-06-02 11:32:47","2025-06-02 11:32:51","2025-06-02 11:32:47","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2407.07052 [eess] version: 1","","/Users/alihaider/Zotero/storage/RYEDF48L/Souza et al. - 2024 - Latent Space Imaging.pdf; /Users/alihaider/Zotero/storage/TJ7WK24Y/2407.html","","","Computer Science - Computer Vision and Pattern Recognition; Electrical Engineering and Systems Science - Image and Video Processing","","","","","","","","","","","","","","","","","","","arXiv:2407.07052","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TMCSXKSK","preprint","2025","Jeon, Hyeon; Park, Jeongin; Shin, Sungbok; Seo, Jinwook","Stop Misusing t-SNE and UMAP for Visual Analytics","","","","10.48550/arXiv.2506.08725","http://arxiv.org/abs/2506.08725","Misuses of t-SNE and UMAP in visual analytics have become increasingly common. For example, although t-SNE and UMAP projections often do not faithfully reflect true distances between clusters, practitioners frequently use them to investigate inter-cluster relationships. In this paper, we bring this issue to the surface and comprehensively investigate why such misuse occurs and how to prevent it. We conduct a literature review of 114 papers to verify the prevalence of the misuse and analyze the reasonings behind it. We then execute an interview study to uncover practitioners' implicit motivations for using these techniques -- rationales often undisclosed in the literature. Our findings indicate that misuse of t-SNE and UMAP primarily stems from limited discourse on their appropriate use in visual analytics. We conclude by proposing future directions and concrete action items to promote more reasonable use of DR.","2025-06-10","2025-06-30 05:17:10","2025-06-30 05:17:13","2025-06-30 05:17:10","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2506.08725 [cs]","","/Users/alihaider/Zotero/storage/RBHFFJRR/Jeon et al. - 2025 - Stop Misusing t-SNE and UMAP for Visual Analytics.pdf; /Users/alihaider/Zotero/storage/JVU84SZZ/2506.html","","","Computer Science - Machine Learning; Computer Science - Human-Computer Interaction","","","","","","","","","","","","","","","","","","","arXiv:2506.08725","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F8J69VGK","preprint","2025","Jha, Rishi; Zhang, Collin; Shmatikov, Vitaly; Morris, John X.","Harnessing the Universal Geometry of Embeddings","","","","10.48550/arXiv.2505.12540","http://arxiv.org/abs/2505.12540","We introduce the first method for translating text embeddings from one vector space to another without any paired data, encoders, or predefined sets of matches. Our unsupervised approach translates any embedding to and from a universal latent representation (i.e., a universal semantic structure conjectured by the Platonic Representation Hypothesis). Our translations achieve high cosine similarity across model pairs with different architectures, parameter counts, and training datasets. The ability to translate unknown embeddings into a different space while preserving their geometry has serious implications for the security of vector databases. An adversary with access only to embedding vectors can extract sensitive information about the underlying documents, sufficient for classification and attribute inference.","2025-06-25","2025-06-30 05:17:24","2025-06-30 05:17:24","2025-06-30 05:17:24","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2505.12540 [cs]","","/Users/alihaider/Zotero/storage/LKGA2679/Jha et al. - 2025 - Harnessing the Universal Geometry of Embeddings.pdf; /Users/alihaider/Zotero/storage/3M88JJI5/2505.html","","","Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2505.12540","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z4G9LKJ9","preprint","2025","Schnaus, Dominik; Araslanov, Nikita; Cremers, Daniel","It's a (Blind) Match! Towards Vision-Language Correspondence without Parallel Data","","","","10.48550/arXiv.2503.24129","http://arxiv.org/abs/2503.24129","The platonic representation hypothesis suggests that vision and language embeddings become more homogeneous as model and dataset sizes increase. In particular, pairwise distances within each modality become more similar. This suggests that as foundation models mature, it may become possible to match vision and language embeddings in a fully unsupervised fashion, i.e. without parallel data. We present the first feasibility study, and investigate conformity of existing vision and language foundation models in the context of unsupervised, or ""blind"", matching. First, we formulate unsupervised matching as a quadratic assignment problem and introduce a novel heuristic that outperforms previous solvers. We also develop a technique to find optimal matching problems, for which a non-trivial match is very likely. Second, we conduct an extensive study deploying a range of vision and language models on four datasets. Our analysis reveals that for many problem instances, vision and language representations can be indeed matched without supervision. This finding opens up the exciting possibility of embedding semantic knowledge into other modalities virtually annotation-free. As a proof of concept, we showcase an unsupervised classifier, which achieves non-trivial classification accuracy without any image-text annotation.","2025-05-29","2025-06-30 05:17:46","2025-06-30 05:17:46","2025-06-30 05:17:46","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2503.24129 [cs]","","/Users/alihaider/Zotero/storage/ETS7WH4N/Schnaus et al. - 2025 - It's a (Blind) Match! Towards Vision-Language Correspondence without Parallel Data.pdf; /Users/alihaider/Zotero/storage/HEEPTCTR/2503.html","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2503.24129","","","","","","","","","","","","","","","","","","","","","","","","","","",""